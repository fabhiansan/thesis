{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 25 08:25:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.01             Driver Version: 535.216.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off | 00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   21C    P8              12W / 230W |    207MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off | 00000000:5E:00.0 Off |                  Off |\n",
      "| 30%   19C    P8               6W / 230W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 5000                Off | 00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   21C    P8               4W / 230W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 5000                Off | 00000000:D8:00.0 Off |                  Off |\n",
      "| 33%   22C    P8              12W / 230W |    229MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1485      C   /home/nugraha/env/bin/python3.10            202MiB |\n",
      "|    3   N/A  N/A      1485      C   /home/nugraha/env/bin/python3.10            226MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import penman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text_to_amr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from text_to_amr import TextToAMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gold-v2\n",
    "# snapshot_download(\n",
    "#     repo_id=\"abdiharyadi/mbart-en-id-smaller-indo-amr-parsing-translated-nafkhan\",\n",
    "#     local_dir=f\"models/mbart-en-id-smaller-indo-amr-parsing-translated-nafkhan\",\n",
    "#     ignore_patterns=[\n",
    "#         \"*log*\",\n",
    "#         \"*checkpoint*\",\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Silver\n",
    "# snapshot_download(\n",
    "#     repo_id=\"abdiharyadi/indoamrbart-ft\",\n",
    "#     local_dir=f\"{PROJECT_ROOT}/AMRBART-id/models\",\n",
    "#     allow_patterns=[\"*mbart-en-id-smaller-fted*\"],\n",
    "#     ignore_patterns=[\n",
    "#         \"*log*\",\n",
    "#         \"*iter*\",\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Gold-v1\n",
    "# snapshot_download(\n",
    "#     repo_id=\"abdiharyadi/indoamrbart-mbart-double-ft\",\n",
    "#     local_dir=f\"{PROJECT_ROOT}/AMRBART-id/models\",\n",
    "#     allow_patterns=[\"*indoamrbart-mbart-fted*\"],\n",
    "#     ignore_patterns=[\n",
    "#         \"*checkpoint*\",\n",
    "#         \"*log*\",\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Saya suka makan ayam.\",\n",
    "    \"Dia pergi ke pasar.\",\n",
    "    \"Dia membeli ayam.\",\n",
    "    \"Dia membeli ayam itu.\",\n",
    "    \"Dia membeli ayam itu di pasar.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah di pagi hari.\",\n",
    "    \"Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah di pagi hari yang cerah.\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 08:26:16.612883: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-25 08:26:16.615763: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-25 08:26:16.668353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-25 08:26:17.437131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from model_interface.tokenization_bart import AMRBartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, AutoConfig\n",
    "import torch\n",
    "import penman\n",
    "\n",
    "class TextToAMRSan:\n",
    "    def __init__(self, model_path=\"./models/mbart-en-id-smaller-indo-amr-parsing-translated-nafkhan\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load config\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        \n",
    "        # Initialize tokenizer with correct parameters\n",
    "        self.tokenizer = AMRBartTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            use_fast=False\n",
    "        )\n",
    "        \n",
    "        # Initialize model with config\n",
    "        self.model = MBartForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            config=self.config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Set important parameters\n",
    "        self.max_src_length = 400\n",
    "        self.max_gen_length = 1024\n",
    "        self.num_beams = 5\n",
    "        \n",
    "    def __call__(self, sentence: str) -> penman.Graph:\n",
    "        # Prepare input with AMR special tokens\n",
    "        raw_txt_ids = self.tokenizer(\n",
    "            sentence,\n",
    "            max_length=self.max_src_length,\n",
    "            padding=False,\n",
    "            truncation=True\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # Add AMR special tokens\n",
    "        txt_ids = [raw_txt_ids[:self.max_src_length-3] + [\n",
    "            self.tokenizer.amr_bos_token_id,\n",
    "            self.tokenizer.mask_token_id,\n",
    "            self.tokenizer.amr_eos_token_id\n",
    "        ]]\n",
    "        \n",
    "        # Pad and convert to tensor\n",
    "        txt_ids = self.tokenizer.pad(\n",
    "            {\"input_ids\": txt_ids},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        preds = self.model.generate(\n",
    "            txt_ids[\"input_ids\"],\n",
    "            max_length=self.max_gen_length,\n",
    "            num_beams=self.num_beams,\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id=self.tokenizer.amr_bos_token_id,\n",
    "            eos_token_id=self.tokenizer.amr_eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            no_repeat_ngram_size=0,\n",
    "            min_length=0\n",
    "        )\n",
    "        \n",
    "        # Decode AMR\n",
    "        pred = preds[0]\n",
    "        pred[0] = self.tokenizer.bos_token_id\n",
    "        pred = [\n",
    "            self.tokenizer.eos_token_id if itm == self.tokenizer.amr_eos_token_id else itm\n",
    "            for itm in pred if itm != self.tokenizer.pad_token_id\n",
    "        ]\n",
    "        \n",
    "        graph, status, (lin, backr) = self.tokenizer.decode_amr(\n",
    "            pred,\n",
    "            restore_name_ops=False\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        metadata = {\n",
    "            \"id\": \"0\",\n",
    "            \"annotator\": \"TextToAMRSan\",\n",
    "            \"snt\": sentence\n",
    "        }\n",
    "        graph.metadata = metadata\n",
    "        \n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_amr = TextToAMRSan()\n",
    "\n",
    "# graph = text_to_amr(sentences[0])\n",
    "# print(\"AMR Graph:\")\n",
    "# print(penman.encode(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Saya suka makan ayam.\n",
      "(z0 / suka-01\n",
      "    :ARG0 (z1 / aku)\n",
      "    :ARG1 (z2 / makan-01\n",
      "              :ARG0 z1\n",
      "              :ARG1 (z3 / ayam)))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia pergi ke pasar.\n",
      "(z0 / pergi-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG4 (z2 / pasar))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG3 (z2 / ayam))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam\n",
      "              :mod (z3 / itu)))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam\n",
      "              :mod (z3 / itu))\n",
      "    :ARG3 (z4 / pasar))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\")\n",
      "                            :ARG1-of (z7 / ramai-01))))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))\n",
      "              :ARG1-of (z7 / ramai-01\n",
      "                           :time (z8 / hari\n",
      "                                     :mod (z9 / minggu)))))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))\n",
      "              :ARG1-of (z7 / ramai-01\n",
      "                           :time (z8 / minggu\n",
      "                                     :ARG1-of (z9 / cerah-01)))))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah di pagi hari.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))\n",
      "              :ARG1-of (z7 / ramai-01)\n",
      "              :time (z8 / minggu)\n",
      "              :time (z9 / entitas-tanggal\n",
      "                        :dayperiod (z10 / pagi))))\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta yang ramai pada hari minggu yang cerah di pagi hari yang cerah.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))\n",
      "              :ARG1-of (z7 / kejut-01\n",
      "                           :time (z8 / minggu)\n",
      "                           :time (z9 / entitas-tanggal\n",
      "                                     :dayperiod (z10 / pagi)\n",
      "                                     :ARG1-of (z11 / cerah-01)))))\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    graph = text_to_amr(sentences[i])\n",
    "    graphs.append(graph)\n",
    "    print(\"AMR Graph:\")\n",
    "    print(penman.encode(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AMRAugmenter:\n",
    "    def __init__(self, source='conceptnet', conceptnet_api=\"http://api.conceptnet.io\"):\n",
    "        self.source = source.lower()\n",
    "        if self.source not in ['nltk', 'conceptnet']:\n",
    "            raise ValueError(\"Source must be either 'nltk' or 'conceptnet'\")\n",
    "        self.conceptnet_api = conceptnet_api\n",
    "        self.pred_error_prob = 0.3\n",
    "        self.entity_error_prob = 0.3\n",
    "        # Add new error probabilities\n",
    "        self.circumstance_error_prob = 0.3\n",
    "        self.discourse_error_prob = 0.3\n",
    "        \n",
    "        # Define circumstance roles that can be modified\n",
    "        self.circumstance_roles = {\n",
    "            ':time', ':location', ':manner', ':duration', \n",
    "            ':instrument', ':purpose', ':source', ':destination'\n",
    "        }\n",
    "        \n",
    "        # Define discourse roles that can be modified\n",
    "        self.discourse_roles = {\n",
    "            ':cause', ':condition', ':concession', ':consequence',\n",
    "            ':temporal-before', ':temporal-after', ':temporal-during'\n",
    "        }\n",
    "    \n",
    "    def get_related_words(self, word):\n",
    "        \"\"\"Get related words based on selected source\"\"\"\n",
    "        if self.source == 'nltk':\n",
    "            return self._get_nltk_related_words(word)\n",
    "        return self._get_conceptnet_related_words(word)\n",
    "    \n",
    "    def _get_nltk_related_words(self, word):\n",
    "        \"\"\"Get related words using NLTK WordNet\"\"\"\n",
    "        related_words = []\n",
    "        synsets = wordnet.synsets(word, lang='ind')\n",
    "        \n",
    "        for synset in synsets:\n",
    "            # Add lemma names\n",
    "            related_words.extend([lemma.name() for lemma in synset.lemmas(lang='ind')])\n",
    "            # Add hypernyms\n",
    "            for hypernym in synset.hypernyms():\n",
    "                related_words.extend([lemma.name() for lemma in hypernym.lemmas(lang='ind')])\n",
    "            # Add hyponyms\n",
    "            for hyponym in synset.hyponyms():\n",
    "                related_words.extend([lemma.name() for lemma in hyponym.lemmas(lang='ind')])\n",
    "        \n",
    "        return list(set(related_words))\n",
    "    \n",
    "    def _get_conceptnet_related_words(self, word):\n",
    "        \"\"\"Get related Indonesian words from ConceptNet\"\"\"\n",
    "        url = f\"{self.conceptnet_api}/c/id/{word}\"\n",
    "        response = requests.get(url).json()\n",
    "        related_words = []\n",
    "        \n",
    "        for edge in response.get('edges', []):\n",
    "            if edge['start']['language'] == 'id':\n",
    "                related_words.append(edge['start']['label'])\n",
    "            if edge['end']['language'] == 'id':\n",
    "                related_words.append(edge['end']['label'])\n",
    "                \n",
    "        return list(set(related_words))\n",
    "\n",
    "    def introduce_predicate_error(self, amr_graph):\n",
    "        \"\"\"Introduce errors in predicates\"\"\"\n",
    "        modified_graph = amr_graph.copy()\n",
    "        \n",
    "        for node in modified_graph.nodes():\n",
    "            if random.random() < self.pred_error_prob:\n",
    "                if 'predicate' in modified_graph.nodes[node]:\n",
    "                    current_pred = modified_graph.nodes[node]['predicate']\n",
    "                    alternatives = self.get_related_words(current_pred)\n",
    "                    if alternatives:\n",
    "                        modified_graph.nodes[node]['predicate'] = random.choice(alternatives)\n",
    "        \n",
    "        return modified_graph\n",
    "\n",
    "    def introduce_entity_error(self, amr_graph):\n",
    "        \"\"\"Introduce errors in entities\"\"\"\n",
    "        modified_graph = amr_graph.copy()\n",
    "        \n",
    "        for node in modified_graph.nodes():\n",
    "            if random.random() < self.entity_error_prob:\n",
    "                if 'entity' in modified_graph.nodes[node]:\n",
    "                    current_entity = modified_graph.nodes[node]['entity']\n",
    "                    alternatives = self.get_related_words(current_entity)\n",
    "                    if alternatives:\n",
    "                        modified_graph.nodes[node]['entity'] = random.choice(alternatives)\n",
    "        \n",
    "        return modified_graph\n",
    "\n",
    "    def introduce_circumstance_error(self, amr_graph):\n",
    "        \"\"\"Introduce errors in circumstantial information (time, location, etc.)\"\"\"\n",
    "        modified_graph = amr_graph.copy()\n",
    "        \n",
    "        for source, target, data in modified_graph.edges(data=True):\n",
    "            role = data.get('role', '')\n",
    "            if role in self.circumstance_roles and random.random() < self.circumstance_error_prob:\n",
    "                # Strategy 1: Swap with another circumstance of the same type\n",
    "                edges_same_role = [(s, t, d) for (s, t, d) in modified_graph.edges(data=True) \n",
    "                                 if d.get('role') == role and (s, t) != (source, target)]\n",
    "                if edges_same_role:\n",
    "                    other_source, other_target, _ = random.choice(edges_same_role)\n",
    "                    # Swap targets\n",
    "                    modified_graph.remove_edge(source, target)\n",
    "                    modified_graph.remove_edge(other_source, other_target)\n",
    "                    modified_graph.add_edge(source, other_target, role=role)\n",
    "                    modified_graph.add_edge(other_source, target, role=role)\n",
    "                \n",
    "                # Strategy 2: Change the circumstance type\n",
    "                elif random.random() < 0.5:\n",
    "                    new_role = random.choice(list(self.circumstance_roles - {role}))\n",
    "                    modified_graph.remove_edge(source, target)\n",
    "                    modified_graph.add_edge(source, target, role=new_role)\n",
    "\n",
    "        return modified_graph\n",
    "\n",
    "    def introduce_discourse_error(self, amr_graph):\n",
    "        \"\"\"Introduce errors in discourse links between statements\"\"\"\n",
    "        modified_graph = amr_graph.copy()\n",
    "        \n",
    "        discourse_edges = [(s, t, d) for (s, t, d) in modified_graph.edges(data=True) \n",
    "                          if d.get('role') in self.discourse_roles]\n",
    "        \n",
    "        for source, target, data in discourse_edges:\n",
    "            if random.random() < self.discourse_error_prob:\n",
    "                current_role = data['role']\n",
    "                \n",
    "                # Strategy 1: Change discourse relation type\n",
    "                if random.random() < 0.5:\n",
    "                    new_role = random.choice(list(self.discourse_roles - {current_role}))\n",
    "                    modified_graph.remove_edge(source, target)\n",
    "                    modified_graph.add_edge(source, target, role=new_role)\n",
    "                \n",
    "                # Strategy 2: Reverse the direction of the relation\n",
    "                else:\n",
    "                    modified_graph.remove_edge(source, target)\n",
    "                    modified_graph.add_edge(target, source, role=current_role)\n",
    "        \n",
    "        return modified_graph\n",
    "\n",
    "    def nx_to_penman(self, nx_graph):\n",
    "        \"\"\"Convert NetworkX graph to Penman graph format\"\"\"\n",
    "        triples = []\n",
    "        instance_triples = []\n",
    "        \n",
    "        # First add instance triples\n",
    "        for node in nx_graph.nodes():\n",
    "            if 'predicate' in nx_graph.nodes[node]:\n",
    "                instance_triples.append(Triple(source=str(node), \n",
    "                                            role=':instance',\n",
    "                                            target=nx_graph.nodes[node]['predicate']))\n",
    "            elif 'entity' in nx_graph.nodes[node]:\n",
    "                instance_triples.append(Triple(source=str(node),\n",
    "                                            role=':instance',\n",
    "                                            target=nx_graph.nodes[node]['entity']))\n",
    "        \n",
    "        # Then add relation triples\n",
    "        for source, target, data in nx_graph.edges(data=True):\n",
    "            triples.append(Triple(source=str(source),\n",
    "                                role=f\":{data['role']}\",\n",
    "                                target=str(target)))\n",
    "        \n",
    "        # Combine instance and relation triples\n",
    "        all_triples = instance_triples + triples\n",
    "        \n",
    "        # Create Penman graph\n",
    "        return Graph(all_triples)\n",
    "\n",
    "    def penman_to_nx(self, penman_graph):\n",
    "        \"\"\"Convert Penman graph to NetworkX format\"\"\"\n",
    "        nx_graph = nx.DiGraph()\n",
    "        \n",
    "        # Process instance triples\n",
    "        for triple in penman_graph.instances():\n",
    "            node_id = triple.source\n",
    "            if triple.target.startswith('pred_'):\n",
    "                nx_graph.add_node(node_id, predicate=triple.target[5:])\n",
    "            else:\n",
    "                nx_graph.add_node(node_id, entity=triple.target)\n",
    "        \n",
    "        # Process relation triples\n",
    "        for triple in penman_graph.edges():\n",
    "            if not triple.role.startswith(':instance'):\n",
    "                nx_graph.add_edge(triple.source, triple.target, \n",
    "                                role=triple.role[1:])  # Remove : prefix\n",
    "        \n",
    "        return nx_graph\n",
    "\n",
    "    def augment_amr(self, amr_graph):\n",
    "        \"\"\"Main function to augment AMR graph with errors\"\"\"\n",
    "        # Convert to NetworkX if input is Penman Graph\n",
    "        if isinstance(amr_graph, Graph):\n",
    "            nx_graph = self.penman_to_nx(amr_graph)\n",
    "        else:\n",
    "            nx_graph = amr_graph\n",
    "            \n",
    "        # Perform augmentation with all error types\n",
    "        modified_graph = self.introduce_predicate_error(nx_graph)\n",
    "        modified_graph = self.introduce_entity_error(modified_graph)\n",
    "        modified_graph = self.introduce_circumstance_error(modified_graph)\n",
    "        modified_graph = self.introduce_discourse_error(modified_graph)\n",
    "        \n",
    "        # Convert back to Penman if input was Penman\n",
    "        if isinstance(amr_graph, Graph):\n",
    "            return self.nx_to_penman(modified_graph)\n",
    "        return modified_graph\n",
    "\n",
    "    def visualize_graph(self, graph, title=\"AMR Graph\"):\n",
    "        \"\"\"Visualize AMR graph with labels\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(graph)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(graph, pos, node_color='lightblue', \n",
    "                             node_size=2000, alpha=0.7)\n",
    "        \n",
    "        # Create node labels\n",
    "        node_labels = {}\n",
    "        for node in graph.nodes():\n",
    "            label = []\n",
    "            if 'predicate' in graph.nodes[node]:\n",
    "                label.append(f\"pred: {graph.nodes[node]['predicate']}\")\n",
    "            if 'entity' in graph.nodes[node]:\n",
    "                label.append(f\"ent: {graph.nodes[node]['entity']}\")\n",
    "            node_labels[node] = '\\n'.join(label)\n",
    "        \n",
    "        # Draw edge labels\n",
    "        edge_labels = nx.get_edge_attributes(graph, 'role')\n",
    "        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(graph, pos, edge_color='gray', arrows=True, \n",
    "                             arrowsize=20)\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(graph, pos, node_labels, font_size=10)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter_nltk = AMRAugmenter(source='nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from penman import Graph, Triple, encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('z0', ':instance', 'beli-01'),\n",
       " ('z0', ':ARG0', 'z1'),\n",
       " ('z1', ':instance', 'dia'),\n",
       " ('z0', ':ARG1', 'z2'),\n",
       " ('z2', ':instance', 'ayam'),\n",
       " ('z0', ':ARG3', 'z3'),\n",
       " ('z3', ':instance', 'pasar'),\n",
       " ('z4', ':ARG1', 'z3'),\n",
       " ('z4', ':instance', 'besar-01'),\n",
       " ('z3', ':location', 'z5'),\n",
       " ('z5', ':instance', 'kota'),\n",
       " ('z5', ':wiki', '\"Jakarta\"'),\n",
       " ('z5', ':name', 'z6'),\n",
       " ('z6', ':instance', 'nama'),\n",
       " ('z6', ':op1', '\"Jakarta\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[6].triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph object (top=z0) at 134737383268576>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_graph = augmenter_nltk.augment_amr(graphs[6])\n",
    "augmented_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Dia membeli ayam itu di pasar yang besar di Jakarta.\n",
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam)\n",
      "    :ARG3 (z3 / pasar\n",
      "              :ARG1-of (z4 / besar-01)\n",
      "              :location (z5 / kota\n",
      "                            :wiki \"Jakarta\"\n",
      "                            :name (z6 / nama\n",
      "                                      :op1 \"Jakarta\"))))\n"
     ]
    }
   ],
   "source": [
    "print(encode(graphs[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(z0 / beli-01\n",
      "    :ARG0 (z1 / dia)\n",
      "    :ARG1 (z2 / ayam_pedaging)\n",
      "    :ARG3 (z3 / gedung\n",
      "              :location (z5 / kota\n",
      "                            :name (z6 / diagnosis))\n",
      "              :ARG1-of (z4 / besar-01)))\n"
     ]
    }
   ],
   "source": [
    "print(encode(augmented_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_amr_with_graphviz(triples):\n",
    "    # Create a new directed graph\n",
    "    dot = Digraph(comment='AMR Graph')\n",
    "    dot.attr(rankdir='TB')  # Top to bottom layout\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    nodes = set()\n",
    "    for source, relation, target in triples:\n",
    "        # Add nodes if they don't exist\n",
    "        if source not in nodes:\n",
    "            dot.node(source, source)\n",
    "            nodes.add(source)\n",
    "        if target not in nodes:\n",
    "            dot.node(target, target)\n",
    "            nodes.add(target)\n",
    "        \n",
    "        # Add edge with relation label\n",
    "        dot.edge(source, target, relation)\n",
    "    \n",
    "    # Display the graph\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_viz = visualize_amr_with_graphviz(augmented_graph.triples)\n",
    "graph_viz1 = visualize_amr_with_graphviz(graphs[6].triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amr_graph.png'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_viz.render('amr_graph', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amr_graph1.png'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_viz1.render('amr_graph1', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(graphs)):\n",
    "    augmented_graph = augmenter_nltk.augment_amr(graphs[i])\n",
    "    graph_viz = visualize_amr_with_graphviz(augmented_graph.triples)\n",
    "    graph_viz_original = visualize_amr_with_graphviz(graphs[i].triples)\n",
    "    graph_viz_original.render(f'amr_graph_original_{i}', format='png', cleanup=True)\n",
    "    graph_viz.render(f'amr_graph_{i}', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class AMRToTextSan:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = T5TokenizerFast.from_pretrained(\n",
    "            os.path.join(model_path, 'tokenizer')\n",
    "        )\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            os.path.join(model_path, 'model')\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Set generation parameters\n",
    "        self.max_seq_len_sent = 512\n",
    "        self.num_beams = 5\n",
    "\n",
    "    def __call__(self, amr_string: str) -> str:\n",
    "        # Prepare input\n",
    "        inputs = self.tokenizer(\n",
    "            amr_string,\n",
    "            max_length=self.max_seq_len_sent,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            num_beams=self.num_beams,\n",
    "            max_length=self.max_seq_len_sent,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "amr_to_text = AMRToTextSan(\"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "dia membeli ayam itu di pasar yang besar dia bernamaarg0-of z1-of z2-ayam di pasar besar-01-arg0-of z5-kota besar-01-wiki\n",
      "Generated augmented text:\n",
      "z4 besar-01:AR-0-of z1-dia :AR-0-of z1-dia :AR-0-of z2 ayam-pedaging :AR-0-of z4 besar-01:AR-0-of z4 besar-01:AR-0-of z4 besar-01:AR-0-of z4 besar-01\n"
     ]
    }
   ],
   "source": [
    "generated_text = amr_to_text(encode(graphs[6]))\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n",
    "\n",
    "generated_augmented_text = amr_to_text(encode(augmented_graph))\n",
    "print(\"Generated augmented text:\")\n",
    "print(generated_augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Balon itu ditiup Ilham\n",
      "(z0 / gulir-01\n",
      "    :ARG0 (z1 / orang\n",
      "              :wiki -\n",
      "              :name (z2 / nama\n",
      "                        :op1 \"Ilham\"))\n",
      "    :ARG1 (z3 / balon\n",
      "              :mod (z4 / itu)))\n",
      "Generated: balon itu ditiup oleh Ilham z0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0 orang di wiki----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Presiden Donald Trump menghabiskan bulan pertama masa jabatan keduanya dalam misi luar biasa \n",
      "(z0 / habis-01\n",
      "    :ARG0 (z1 / orang\n",
      "              :wiki \"Donald_Trump\"\n",
      "              :name (z2 / nama\n",
      "                        :op1 \"Donald\"\n",
      "                        :op2 \"Trump\")\n",
      "              :ARG0-of (z3 / punya-peran-org-91\n",
      "                           :ARG2 (z4 / presiden)))\n",
      "    :ARG1 (z5 / bulan\n",
      "              :ord (z6 / entitas-ordinal\n",
      "                       :value 1)\n",
      "              :ord (z7 / entitas-ordinal\n",
      "                       :value 2))\n",
      "    :ARG2 (z8 / misi-01\n",
      "              :mod (z9 / luar-biasa)))\n",
      "Generated: pada bulan pertama masa jabatannya, presiden menghabiskan z3 bulan dalam misi luar biasa z3-or z6-or z7-entitas-ordinal-value 1-or z8-or z7-entitas-ordinal-value 2-or z8-or z8-entitas-ordinal-value 1-or z8-or z8-entitas-ordinal\n",
      "AMR Graph:\n",
      "# ::id 0\n",
      "# ::annotator TextToAMRSan\n",
      "# ::snt Secara teoritis selalu ada kemungkinan bahwa Barat bisa kehilangan resonansinya saat Perang Dunia II dan Perang Dingin semakin menjadi kenangan yang jauh.\n",
      "(z0 / mungkin-01\n",
      "    :ARG1 (z1 / hilang-01\n",
      "              :ARG0 (z2 / negara\n",
      "                        :wiki \"Amerika_Serikat\"\n",
      "                        :name (z3 / nama\n",
      "                                  :op1 \"Barat\"))\n",
      "              :ARG1 (z4 / sesuatu\n",
      "                        :ARG1-of (z5 / atur-01))\n",
      "              :time (z6 / dan\n",
      "                        :op1 (z7 / peristiwa\n",
      "                                 :wiki \"World_War_II\"\n",
      "                                 :name (z8 / nama\n",
      "                                           :op1 \"World\"\n",
      "                                           :op2 \"War\"\n",
      "                                           :op3 \"II\"))\n",
      "                        :op2 (z9 / jadi-01\n",
      "                                 :ARG1 (z10 / peristiwa)\n",
      "                                 :ARG2 (z11 / ingat-01\n",
      "                                            :ARG1-of (z12 / jauh-01))\n",
      "                                 :time (z13 / semakin-01)))\n",
      "              :time (z14 / selalu)))\n",
      "Generated: z0 mungkin kehilangan resonansinya saat z2 dan z9 menjadi suatu peristiwa:arg1-of z1-of z5-atur-01-arg1-of z12-atur-01-arg1-of z12-atur-01-arg1-of z12-atur-01-arg1-of z12-atur-01-arg1-of z12-atur-01-arg1-of z12-atur-01\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    graph = text_to_amr(sentences[i])\n",
    "    print(\"AMR Graph:\")\n",
    "    a = penman.encode(graph)\n",
    "    generated_text = amr_to_text(a)\n",
    "    print(\"Generated:\", generated_text)\n",
    "\n",
    "# sent1 = sentences[0]\n",
    "    \n",
    "# # Text -> AMR -> Text\n",
    "# amr_graph = text_to_amr(sent1)\n",
    "# amr_string = penman.encode(amr_graph)\n",
    "# generated_text = amr_to_text(amr_string)\n",
    "\n",
    "# print(\"Original:\", sent1)\n",
    "# print(\"AMR:\", amr_string)\n",
    "# print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sisa uang yang aku miliki saat ini hanya berjumlah sepuluh ribu rupiah\n",
      "AMR: (z0 / sisa-01\n",
      "    :ARG1 (z1 / uang\n",
      "              :ARG1-of (z2 / punya-01\n",
      "                           :ARG0 (z3 / aku)\n",
      "                           :time (z4 / saat\n",
      "                                     :mod (z5 / ini))))\n",
      "    :ARG2 (z6 / kuantitas-moneter\n",
      "              :quant 10\n",
      "              :unit (z7 / dolar)\n",
      "              :mod (z8 / hanya)))\n",
      "Generated: z1, z2, z3, aku punya uang :arg1-of z2, z3, aku punya uang :arg1-of z6, kuantitas-moneter :quant 10-unit z7 dolar, z8 hanya saat ini.\n"
     ]
    }
   ],
   "source": [
    "sent1 = sentences[1]\n",
    "    \n",
    "# Text -> AMR -> Text\n",
    "amr_graph = text_to_amr(sent1)\n",
    "\n",
    "amr_graph.metadata = {}\n",
    "\n",
    "amr_string = penman.encode(amr_graph)\n",
    "\n",
    "\n",
    "generated_text = amr_to_text(amr_string)\n",
    "\n",
    "print(\"Original:\", sent1)\n",
    "print(\"AMR:\", amr_string)\n",
    "print(\"Generated:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sisa uang yang aku miliki saat ini hanya berjumlah sepuluh ribu rupiah'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_graph.metadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
