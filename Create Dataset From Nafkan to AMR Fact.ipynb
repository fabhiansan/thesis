{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fba88f-5f08-4a0f-973c-fe02fa1ecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BOT_TOKEN = '6731314774:AAFNdgII0MX5Kdohl4aUq8cz9ArEsvdIdWw'\n",
    "CHAT_ID = '1653491203' \n",
    "\n",
    "def send_telegram_message(message):\n",
    "    url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'\n",
    "    data = {\n",
    "        'chat_id': CHAT_ID,\n",
    "        'text': message\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, data=data)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error sending message: {e}\")\n",
    "\n",
    "send_telegram_message(\"bot ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c618927c-da4d-44a2-8ac4-33bf2d9b3a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jdk',\n",
       " 'splade',\n",
       " 'AMRBART',\n",
       " '.local',\n",
       " '.ipynb_checkpoints',\n",
       " 'amr-tst-indo',\n",
       " '.ssh',\n",
       " '.git-credentials',\n",
       " '.python_history',\n",
       " 'git clone.txt',\n",
       " '__MACOSX',\n",
       " '.rustup',\n",
       " '.keras',\n",
       " 'conceptnet-assertions-5.7.0.csv',\n",
       " 'dataset',\n",
       " 'amr_parser',\n",
       " 'data-builder',\n",
       " '.bash_history',\n",
       " '.bash_logout',\n",
       " '.profile',\n",
       " '.wget-hsts',\n",
       " '.nv',\n",
       " '.jupyter',\n",
       " '.demo.db.lock',\n",
       " '.conda',\n",
       " '??',\n",
       " '.config',\n",
       " '.gitconfig',\n",
       " '.cache',\n",
       " 'shared_data',\n",
       " '.cargo',\n",
       " 'thesis',\n",
       " 'Dataset',\n",
       " '.npm',\n",
       " '.ipython',\n",
       " 'nltk_data',\n",
       " '.zshenv',\n",
       " '.bashrc',\n",
       " 'stanza_resources']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc4a51e-74f7-4158-9f30-a6d8466628a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['devdataset', 'devdataset_amr', 'traindataset', 'testdataset']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_dir = \"thesis\"\n",
    "# os.chdir(new_dir)\n",
    "\n",
    "indosum_files = os.listdir(\"../dataset/indosum\")\n",
    "indosum_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ba6290-af99-45cb-b911-e9e92df4781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_concat_amr2 = \"../dataset/ds/amrbart-id-concat-train-amr2/train.jsonl\"\n",
    "path_train_concat_amr3 = \"../dataset/ds/amrbart-id-concat-train-amr3/train.jsonl\"\n",
    "path_train_set_hki = \"../dataset/Dataset Berita Palsu/train_set_hki.csv\"\n",
    "path_indosum = \"../dataset/indosum/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8004443f-6d63-4d4d-b674-152e9ce92903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import penman\n",
    "from utils.pointer_to_penman import convert_amr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48707d4-f3a5-421d-aa5c-b9fd2df159f9",
   "metadata": {},
   "source": [
    "# dataset nafkhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2369933-7fc7-4b64-beae-e1e64e7fae86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Content</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Source_Claim</th>\n",
       "      <th>Source_link</th>\n",
       "      <th>Relevant_urls</th>\n",
       "      <th>Article_inspected</th>\n",
       "      <th>LLM_key_points</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8343</td>\n",
       "      <td>fake</td>\n",
       "      <td>28/11/2019</td>\n",
       "      <td>Restoran Sederhana Ulang Tahun Ke-15 Membagika...</td>\n",
       "      <td>Beredar melalui Whatsapp pesan yang berisikan ...</td>\n",
       "      <td>Fabricated Content</td>\n",
       "      <td>whatsapp.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://www.liputan6.com/cek-fakta/read/4121...</td>\n",
       "      <td>Liputan6.com, Jakarta - Restoran Padang Seder...</td>\n",
       "      <td>- Restoran Padang Sederhana dikabarkan membagi...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>SOSIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10567</td>\n",
       "      <td>fake</td>\n",
       "      <td>17/11/2020</td>\n",
       "      <td>Video Pemindahan Kubur Imam Samudera akibat Pr...</td>\n",
       "      <td>Beredar sebuah video oleh akun Youtube Lida Ch...</td>\n",
       "      <td>Misleading Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>https://archive.vn/ydSmW</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/ObzM...</td>\n",
       "      <td>Beredar sebuah video melalui pesan berantai W...</td>\n",
       "      <td>- Video yang beredar mengklaim memperlihatkan ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>LAINNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18146</td>\n",
       "      <td>fake</td>\n",
       "      <td>26/07/2023</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>https://archive.cob.web.id/archive/1689934479....</td>\n",
       "      <td>['https://turnbackhoax.id/2023/07/26/salah-pdi...</td>\n",
       "      <td>Hasil periksa fakta Ummul Hidayah. Unggahan v...</td>\n",
       "      <td>- Unggahan video dengan klaim \"PDIP PANIK TOTA...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443</td>\n",
       "      <td>fake</td>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>Indonesia dan Australia Perang, Terdengar Suar...</td>\n",
       "      <td>Bener ga sih Indonesia dan Australia bakal perang</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Tiktok</td>\n",
       "      <td>http://archive.cob.web.id/archive/1673546109.2...</td>\n",
       "      <td>['https://turnbackhoax.id/2023/01/16/salah-ind...</td>\n",
       "      <td>Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...</td>\n",
       "      <td>- Video yang mengklaim bahwa terjadi perang an...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>HUKUM DAN KRIMINALITAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12455</td>\n",
       "      <td>fake</td>\n",
       "      <td>23/08/2021</td>\n",
       "      <td>Jokowi Lepas Baju PDIP, Megawati Murka</td>\n",
       "      <td>JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>archive.vn/kW5nJ</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/nN94...</td>\n",
       "      <td>Kanal PENA ISTANA membagikan video itu pada 1...</td>\n",
       "      <td>- Video yang menyatakan bahwa Presiden Jokowi ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Label        Date                                              Claim  \\\n",
       "0   8343  fake  28/11/2019  Restoran Sederhana Ulang Tahun Ke-15 Membagika...   \n",
       "1  10567  fake  17/11/2020  Video Pemindahan Kubur Imam Samudera akibat Pr...   \n",
       "2  18146  fake  26/07/2023  PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...   \n",
       "3  16443  fake  16/01/2023  Indonesia dan Australia Perang, Terdengar Suar...   \n",
       "4  12455  fake  23/08/2021             Jokowi Lepas Baju PDIP, Megawati Murka   \n",
       "\n",
       "                                             Content       Classification  \\\n",
       "0  Beredar melalui Whatsapp pesan yang berisikan ...   Fabricated Content   \n",
       "1  Beredar sebuah video oleh akun Youtube Lida Ch...   Misleading Content   \n",
       "2  PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...  Manipulated Content   \n",
       "3  Bener ga sih Indonesia dan Australia bakal perang  Manipulated Content   \n",
       "4  JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...  Manipulated Content   \n",
       "\n",
       "   Source_Claim                                        Source_link  \\\n",
       "0  whatsapp.com                                                NaN   \n",
       "1   Youtube.com                           https://archive.vn/ydSmW   \n",
       "2       Youtube  https://archive.cob.web.id/archive/1689934479....   \n",
       "3        Tiktok  http://archive.cob.web.id/archive/1673546109.2...   \n",
       "4   Youtube.com                                   archive.vn/kW5nJ   \n",
       "\n",
       "                                       Relevant_urls  \\\n",
       "0  ['https://www.liputan6.com/cek-fakta/read/4121...   \n",
       "1  ['https://www.medcom.id/telusur/cek-fakta/ObzM...   \n",
       "2  ['https://turnbackhoax.id/2023/07/26/salah-pdi...   \n",
       "3  ['https://turnbackhoax.id/2023/01/16/salah-ind...   \n",
       "4  ['https://www.medcom.id/telusur/cek-fakta/nN94...   \n",
       "\n",
       "                                   Article_inspected  \\\n",
       "0   Liputan6.com, Jakarta - Restoran Padang Seder...   \n",
       "1   Beredar sebuah video melalui pesan berantai W...   \n",
       "2   Hasil periksa fakta Ummul Hidayah. Unggahan v...   \n",
       "3   Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...   \n",
       "4   Kanal PENA ISTANA membagikan video itu pada 1...   \n",
       "\n",
       "                                      LLM_key_points         Tone  \\\n",
       "0  - Restoran Padang Sederhana dikabarkan membagi...      NEUTRAL   \n",
       "1  - Video yang beredar mengklaim memperlihatkan ...  NOT-NEUTRAL   \n",
       "2  - Unggahan video dengan klaim \"PDIP PANIK TOTA...  NOT-NEUTRAL   \n",
       "3  - Video yang mengklaim bahwa terjadi perang an...  NOT-NEUTRAL   \n",
       "4  - Video yang menyatakan bahwa Presiden Jokowi ...  NOT-NEUTRAL   \n",
       "\n",
       "                    Topic  \n",
       "0                  SOSIAL  \n",
       "1                 LAINNYA  \n",
       "2                 POLITIK  \n",
       "3  HUKUM DAN KRIMINALITAS  \n",
       "4                 POLITIK  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# df_train_concat_amr2 = pd.read_json(path_train_concat_amr2, lines=True)\n",
    "# df_train_concat_amr2.head()\n",
    "# df_train_concat_amr3 = pd.read_json(path_train_concat_amr3, lines=True)\n",
    "# df_train_concat_amr3.head()\n",
    "df_train_set_hki = pd.read_csv(path_train_set_hki)\n",
    "df_train_set_hki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10214ac2-dbbf-416f-8d42-13a1f82d581a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6241"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_set_hki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bd21a-e020-4280-9eb0-5958925aba1a",
   "metadata": {},
   "source": [
    "# dataset indohoax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7491b828-4229-462c-bef7-f2c4da67bafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                               17483\n",
       "Label                                                             fake\n",
       "Date                                                        13/04/2023\n",
       "Claim                Pesulap Merah Meninggal Dunia Usai Adu Kesakti...\n",
       "Content              innalilahi wainnailaihi hirojiun\\nhari ini dik...\n",
       "Classification                                      Fabricated Content\n",
       "Source_Claim                                                   Youtube\n",
       "Source_link                                   https://archive.fo/limxu\n",
       "Relevant_urls        ['https://www.liputan6.com/showbiz/read/526133...\n",
       "Article_inspected     Liputan6.com, Jakarta - Media sosial tengah h...\n",
       "LLM_key_points       - Terdapat kabar di media sosial yang menyebut...\n",
       "Tone                                                       NOT-NEUTRAL\n",
       "Topic                                                          HIBURAN\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_train_set_hki.iloc[100]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4500da2-beb7-41b1-accd-e78a3daa2fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pesulap Merah Meninggal Dunia Usai Adu Kesaktian Dengan Dukun'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['Claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014687ca-fdee-487a-82f1-c4a53b66ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "innalilahi wainnailaihi hirojiun\n",
      "hari ini dikabarkan pesulap merah atau\n",
      "Marsel radival meninggal dunia seusai adu kesaktian dengan dukun\n"
     ]
    }
   ],
   "source": [
    "print(a['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68e9350c-ddde-42b8-bc29-c048da3ba3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Liputan6.com, Jakarta - Media sosial tengah heboh dengan pemberitaan Marcel Radhival alias Pesulap Merah. Ia disebut-sebut telah meninggal dunia.\n",
      "Dari narasi yang beredar, Pesulap Merah disebut meninggal dunia usai beradu kesaktian dengan seorang dukun. Sebagaimana diketahui, selama ini Pesulap Merah memang begitu lantang melawan kebohongan-kebohongan ilmu perdukunan.\n",
      "Marcel kemudian bereaksi dengan berita hoax yang ada. Di Instagram-nya, Pesulap Merah mengunggah sebuah tangkapan layar sebuah video dari kanal YouTube yang memberitakan dirinya meninggal dunia.\n",
      "Advertisement\n",
      "\"INNALILLAHI.. MARSEL RADIVAL MENINGGAL DUNIA USAI ADU KESAKTIAN DENGAN DUKUN,\" begitu judul yang tertulis dalam video di kanal TC Media yang diunggah ulang di Instagram Marcel beberapa waktu lalu.\n",
      "* Follow Official WhatsApp Channel Liputan6.com untuk mendapatkan berita-berita terkini dengan mengklik tautan ini.\n",
      "Banyak Konten Hoax Disebar di Grup-Grup\n",
      "Menyertai unggahan itu, Pesulap Merah menuliskan keterangan yang bernada meledek si berita hoax tersebut. Ia juga khawatir bahwa konten-konten seperti ini lah yang sangat mungkin bisa menyesatkan.\n",
      "\"Waaaah sekarang saya jadi hantu chuakakakssss terrrrrrrrr. Kalo dishare ke tiktok atau group keluarga pasti pada percaya nih sama berita sampah beginian,\" tulis Pesulap Merah.\n",
      "Advertisement\n",
      "Masyarakat Indonesia Minim Literasi\n",
      "Lebih lanjut, pesulap merah menyebut bahwa ini menjadi bukti bagaimana masyarakat Indonesia masih sangat minim literasi.\n",
      "\"bhahaha parah memang, dari kasus saya difitnah (lagi) ini, membuktikan banyak masyarakat indonesia yang MINIM LITERASI alias GAMPANG PERCAYAAN TERHADAP HAL YANG TIDAK ADA BUKTINYA. #PesulapMerah #ILMUMERAH,\" sambung Pesulap Merah lagi.\n",
      "Banyak Warganet yang Hampir Percaya\n",
      "Di kolom komentar, banyak juga warganet yang mengaku hampir percaya dengan konten-konten hoax tersebut.\n",
      "\"wallahi ana kaget, ana kira bener tau tau ente yg post. panjang umur insyaallah pesulap merah!\" tulis @ali.habsyi.\n",
      "\"Bang @marcelradhival1 gw debat Amal bapak gw gara\" dia nonton ytb soal lu yg meninggal. Gw udah bilang lu masih idup gw tunjukkin akun Instagram lu die keukeuh gk percaya,\" timpal @ladyrocker.nikeardilla.\n",
      "* Fakta atau Hoaks? Untuk mengetahui kebenaran informasi yang beredar, silakan WhatsApp ke nomor Cek Fakta Liputan6.com 0811 9787 670 hanya dengan ketik kata kunci yang diinginkan.\n",
      "AdvertisementCek Fakta: Benarkah Pesulap Merah Meninggal Dunia Usai Adu Kesaktian .... PIKIRAN RAKYAT - Beredar kabar di media sosial, yang menyebutkan Pesulap Merah atau pemilik nama Marcel Radhival meninggal dunia usai adu kekuatan dengan dukun. Tampak tangkapan layar YouTube dengan thumbnail menyebutkan jika Pesulap Merah kalah dari dukun hingga meninggal dunia. Cek Fakta: Pesulap Merah Meninggal Dunia Usai Adu Kesaktian dengan Dukun\n",
      "Dream - Laman media sosial, khususnya Youtube, geger dengan kabar Marcel Radhival atau yang dikenal dengan nama Pesulap Merah meninggal dunia. Informasi yang beredar menyebutkan pesulap merah itu meninggal usai adu kesaktian dengan seorang dukun.\n",
      "Informasi tersebut diketahui pertama kali beredar di channel YouTubeTC Media.\n",
      "\"INNALILLAHI.. MARSEL RADIVAL MENINGGAL DUNIA USAI ADU KESAKTIAN DENGAN DUKUN,\" demikian narasi postingan tersebut.\n",
      "-\n",
      "Rumus apa yang pertama kali ditunjukkan kepada Kenkulus dalam video viralnya? Seperti dalam salah satu video, Ken tampak dites dengan rumus-rumus. Sang ayah memperlihatkan sebuah rumus dalam kertas biru, lalu Ken menyebut \"piramid\". Ternyata benar, karena itu merupakan rumus untuk menghitung piramid.\n",
      "-\n",
      "Siapa yang memulai percakapan pertama di media sosial? Rupanya Maulana lah yang menyapa duluan dengan mengirimkan pesan menyapa Chan dengan sangat singkat. ‘heii,\" tulis Maulana kepada Chan.\n",
      "-\n",
      "Kapan video tersebut direkam? Momen yang terjadi pada Oktober 2015 silam itu pun kembali viral dan mendapat sorotan dari netizen yang melihatnya.\n",
      "Pada thumbnail video berdurasi empat menit tersebut, terlihat foto Pesulap Merah, serta pembawa acara berita dan juga potongan mobil ambulans yang diduga membawa jenazah Pesulap Merah.\n",
      "Dalam narasinya menerangkan, pesulap merah meninggal akibat beradu pembuktian ilmu santet di Jakarta, dengan pemuda asal suku pedalaman Kalimantan Tengah.\n",
      "Saat pembuktian ilmu Pesulap Merah biasa saja, namun saat kembali ke rumah ia merasa mual dan akhirnya meninggal saat dibawa ke rumah sakit.\n",
      "Lalu, benarkah kabar Pesulap Merah meninggal dunia karena adu kesaktian dengan dukun?\n",
      "Melalui akun instagram resminya @marcelradhival1, Pesulap Merah mengklarifikasi bahwa informasi yang disebarkan oleh kanal Youtube tersebut adalah hoaks.\n",
      "“Waaaah sekarang saya jadi hantu chuakakakssss terrrrrrrrr\n",
      "Kalo dishare ke tiktok atau group keluarga pasti pada percaya nih sama berita sampah beginian. bhahaha parah memang, dari kasus saya difitnah (lagi) ini, membuktikan banyak masyarakat indonesia yang MINIM LITERASI alias GAMPANG PERCAYAAN TERHADAP HAL YANG TIDAK ADA BUKTINYA”, tulisnya.\n",
      "Cobain For You Page (FYP) Yang kamu suka ada di sini,\n",
      "lihat isinya\n",
      "Wanita WNI ditemukan tewas dengan luka tusuk, suami dilaporkan bunuh diri.\n",
      "Baca SelengkapnyaIbu terduga pelaku pembunuhan anak kandungnya di Bekasi sempat tertawa saat diperiksa.\n",
      "Baca SelengkapnyaAwalnya banyak pemirsa, terutama yang dari Indonesia, menduga orangtua Upin dan Ipin meninggal dunia. Namun tidak diketahui penyebab mereka meninggal.\n",
      "Baca Selengkapnyavideo untuk kamu.\n",
      "Faktanya, konsep makam ini telah ada sejak lama. Yuk, simak makam-makam tertua yang ada di dunia!\n",
      "Baca SelengkapnyaOjol temukan kerangka manusia di lahan kosong di Makassar, diduga berjenis kelamin perempuan.\n",
      "Baca SelengkapnyaBulog telah menyelidiki video viral yang menunjukkan buruh berguling-guling sambil mandi beras.\n",
      "Baca SelengkapnyaKecelakaan tunggal di jurang kawasan Bromo, 4 orang meninggal dunia, 5 orang yang selamat dibawa ke rumah sakit.\n",
      "Baca SelengkapnyaSelalu menolak diajak bertemu, pemuda di Lamongan gagal nikah pasangan tak hadir saat hari H.\n",
      "Baca SelengkapnyaBeberapa orang mungkin mengalami gangguan kecemasan yang bisa semakin parah. Yuk, simak cara mengatasinya!\n",
      "Baca SelengkapnyaKalian saat memakai celana kepanjangan pasti suka melipat bagian bawahnya biar terlihat rapi. Nah, mungkin tips ini yang sedang kalian cari.\n",
      "Baca Selengkapnya\n"
     ]
    }
   ],
   "source": [
    "print(a['Article_inspected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40628d35-a3f3-496b-abee-a0bb6ad78d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Terdapat kabar di media sosial yang menyebutkan bahwa Marcel Radhival, yang dikenal sebagai Pesulap Merah, telah meninggal dunia setelah beradu kesaktian dengan seorang dukun.', 'Kabar tersebut pertama kali beredar di kanal YouTube TC Media dengan judul yang menyatakan bahwa Pesulap Merah meninggal dunia akibat adu kesaktian.', 'Pesulap Merah mengklarifikasi melalui akun Instagram-nya bahwa informasi tersebut adalah hoaks dan tidak benar.', 'Dalam unggahannya, Pesulap Merah menyatakan bahwa berita tersebut adalah \"berita sampah\" dan menunjukkan bahwa masyarakat Indonesia masih minim literasi, sehingga mudah percaya pada informasi yang tidak memiliki bukti.', 'Banyak warganet yang hampir percaya dengan berita hoax tersebut, menunjukkan reaksi kaget dan mengaku mendengar kabar tersebut dari orang lain.']\n"
     ]
    }
   ],
   "source": [
    "b = a['LLM_key_points'].split(\"\\n- \")\n",
    "b[0] = b[0].lstrip(\"- \")\n",
    "# b = a['LLM_key_points']\n",
    "print(b)\n",
    "# for i in b:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab78a2ee-6720-4393-9210-5de0a1b900b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_train_set_hki)\n",
    "df['x'] = None\n",
    "# df = pd.DataFrame({'x': [None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "120a3c57-4f57-4970-8440-6fa02cc23483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Content</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Source_Claim</th>\n",
       "      <th>Source_link</th>\n",
       "      <th>Relevant_urls</th>\n",
       "      <th>Article_inspected</th>\n",
       "      <th>LLM_key_points</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Topic</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8343</td>\n",
       "      <td>fake</td>\n",
       "      <td>28/11/2019</td>\n",
       "      <td>Restoran Sederhana Ulang Tahun Ke-15 Membagika...</td>\n",
       "      <td>Beredar melalui Whatsapp pesan yang berisikan ...</td>\n",
       "      <td>Fabricated Content</td>\n",
       "      <td>whatsapp.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://www.liputan6.com/cek-fakta/read/4121...</td>\n",
       "      <td>Liputan6.com, Jakarta - Restoran Padang Seder...</td>\n",
       "      <td>- Restoran Padang Sederhana dikabarkan membagi...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>SOSIAL</td>\n",
       "      <td>[Terdapat kabar di media sosial yang menyebutk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10567</td>\n",
       "      <td>fake</td>\n",
       "      <td>17/11/2020</td>\n",
       "      <td>Video Pemindahan Kubur Imam Samudera akibat Pr...</td>\n",
       "      <td>Beredar sebuah video oleh akun Youtube Lida Ch...</td>\n",
       "      <td>Misleading Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>https://archive.vn/ydSmW</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/ObzM...</td>\n",
       "      <td>Beredar sebuah video melalui pesan berantai W...</td>\n",
       "      <td>- Video yang beredar mengklaim memperlihatkan ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>LAINNYA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18146</td>\n",
       "      <td>fake</td>\n",
       "      <td>26/07/2023</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>https://archive.cob.web.id/archive/1689934479....</td>\n",
       "      <td>['https://turnbackhoax.id/2023/07/26/salah-pdi...</td>\n",
       "      <td>Hasil periksa fakta Ummul Hidayah. Unggahan v...</td>\n",
       "      <td>- Unggahan video dengan klaim \"PDIP PANIK TOTA...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443</td>\n",
       "      <td>fake</td>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>Indonesia dan Australia Perang, Terdengar Suar...</td>\n",
       "      <td>Bener ga sih Indonesia dan Australia bakal perang</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Tiktok</td>\n",
       "      <td>http://archive.cob.web.id/archive/1673546109.2...</td>\n",
       "      <td>['https://turnbackhoax.id/2023/01/16/salah-ind...</td>\n",
       "      <td>Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...</td>\n",
       "      <td>- Video yang mengklaim bahwa terjadi perang an...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>HUKUM DAN KRIMINALITAS</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12455</td>\n",
       "      <td>fake</td>\n",
       "      <td>23/08/2021</td>\n",
       "      <td>Jokowi Lepas Baju PDIP, Megawati Murka</td>\n",
       "      <td>JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>archive.vn/kW5nJ</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/nN94...</td>\n",
       "      <td>Kanal PENA ISTANA membagikan video itu pada 1...</td>\n",
       "      <td>- Video yang menyatakan bahwa Presiden Jokowi ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>3248</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Moral Politik Para Menteri Jelang 2024: Manuve...</td>\n",
       "      <td>Menyoal Moral Politik di Tengah Manuver Para M...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://nasional.kompas.com/read/2022/05/22/105...</td>\n",
       "      <td>['https://nasional.kompas.com/read/2022/05/22/...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Sejumlah menteri dalam ...</td>\n",
       "      <td>- Sejumlah menteri dalam pemerintahan Presiden...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>2374</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Krisis Politik Israel: Parlemen Bubar, Netanya...</td>\n",
       "      <td>Krisis Politik Israel: Parlemen Bubar, Bagaima...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://www.kompas.com/global/read/2022/06/29/1...</td>\n",
       "      <td>['https://www.kompas.com/global/read/2022/06/2...</td>\n",
       "      <td>YERUSALEM, KOMPAS.com - Parlemen Israel diper...</td>\n",
       "      <td>- Parlemen Israel diperkirakan akan bubar pada...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>3468</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Bawaslu: Kontrak Politik Prabowo dengan KSPI S...</td>\n",
       "      <td>Tanggapan Bawaslu Soal Kontrak Politik Prabowo...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://nasional.kompas.com/read/2018/05/02/214...</td>\n",
       "      <td>['https://nasional.kompas.com/read/2018/05/02/...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Ketua umum Partai Gerin...</td>\n",
       "      <td>- Prabowo Subianto, Ketua Umum Partai Gerindra...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>271</td>\n",
       "      <td>real</td>\n",
       "      <td>24/12/2021</td>\n",
       "      <td>KedaiKOPI: Elektabilitas Prabowo Tertinggi Ber...</td>\n",
       "      <td>KedaiKOPI Sebut Elektabilitas Tinggi Prabowo K...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20211224...</td>\n",
       "      <td>['https://www.beritasatu.com/nasional/889697/s...</td>\n",
       "      <td>Survei KedaiKopi: Elektabilitas Prabowo Terti...</td>\n",
       "      <td>- Lembaga survei KedaiKopi merilis hasil surve...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6240</th>\n",
       "      <td>4147</td>\n",
       "      <td>real</td>\n",
       "      <td>14/06/2022</td>\n",
       "      <td>Sekjen PDIP Hasto Kristiyanto: Jangan Terbuai ...</td>\n",
       "      <td>Sekjen PDIP Peringatkan Kader Partainya Tak Te...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TEMPO</td>\n",
       "      <td>https://nasional.tempo.co/read/1601740/sekjen-...</td>\n",
       "      <td>['https://nasional.tempo.co/read/1601740/sekje...</td>\n",
       "      <td>TEMPO.CO, Jakarta - Sekretaris Jenderal PDIP ...</td>\n",
       "      <td>- Sekretaris Jenderal PDIP, Hasto Kristiyanto,...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6241 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Label        Date  \\\n",
       "0      8343  fake  28/11/2019   \n",
       "1     10567  fake  17/11/2020   \n",
       "2     18146  fake  26/07/2023   \n",
       "3     16443  fake  16/01/2023   \n",
       "4     12455  fake  23/08/2021   \n",
       "...     ...   ...         ...   \n",
       "6236   3248  real  22/02/2023   \n",
       "6237   2374  real  22/02/2023   \n",
       "6238   3468  real  22/02/2023   \n",
       "6239    271  real  24/12/2021   \n",
       "6240   4147  real  14/06/2022   \n",
       "\n",
       "                                                  Claim  \\\n",
       "0     Restoran Sederhana Ulang Tahun Ke-15 Membagika...   \n",
       "1     Video Pemindahan Kubur Imam Samudera akibat Pr...   \n",
       "2     PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...   \n",
       "3     Indonesia dan Australia Perang, Terdengar Suar...   \n",
       "4                Jokowi Lepas Baju PDIP, Megawati Murka   \n",
       "...                                                 ...   \n",
       "6236  Moral Politik Para Menteri Jelang 2024: Manuve...   \n",
       "6237  Krisis Politik Israel: Parlemen Bubar, Netanya...   \n",
       "6238  Bawaslu: Kontrak Politik Prabowo dengan KSPI S...   \n",
       "6239  KedaiKOPI: Elektabilitas Prabowo Tertinggi Ber...   \n",
       "6240  Sekjen PDIP Hasto Kristiyanto: Jangan Terbuai ...   \n",
       "\n",
       "                                                Content       Classification  \\\n",
       "0     Beredar melalui Whatsapp pesan yang berisikan ...   Fabricated Content   \n",
       "1     Beredar sebuah video oleh akun Youtube Lida Ch...   Misleading Content   \n",
       "2     PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...  Manipulated Content   \n",
       "3     Bener ga sih Indonesia dan Australia bakal perang  Manipulated Content   \n",
       "4     JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...  Manipulated Content   \n",
       "...                                                 ...                  ...   \n",
       "6236  Menyoal Moral Politik di Tengah Manuver Para M...                 TRUE   \n",
       "6237  Krisis Politik Israel: Parlemen Bubar, Bagaima...                 TRUE   \n",
       "6238  Tanggapan Bawaslu Soal Kontrak Politik Prabowo...                 TRUE   \n",
       "6239  KedaiKOPI Sebut Elektabilitas Tinggi Prabowo K...                 TRUE   \n",
       "6240  Sekjen PDIP Peringatkan Kader Partainya Tak Te...                 TRUE   \n",
       "\n",
       "      Source_Claim                                        Source_link  \\\n",
       "0     whatsapp.com                                                NaN   \n",
       "1      Youtube.com                           https://archive.vn/ydSmW   \n",
       "2          Youtube  https://archive.cob.web.id/archive/1689934479....   \n",
       "3           Tiktok  http://archive.cob.web.id/archive/1673546109.2...   \n",
       "4      Youtube.com                                   archive.vn/kW5nJ   \n",
       "...            ...                                                ...   \n",
       "6236        KOMPAS  http://nasional.kompas.com/read/2022/05/22/105...   \n",
       "6237        KOMPAS  http://www.kompas.com/global/read/2022/06/29/1...   \n",
       "6238        KOMPAS  http://nasional.kompas.com/read/2018/05/02/214...   \n",
       "6239           CNN  https://www.cnnindonesia.com/nasional/20211224...   \n",
       "6240         TEMPO  https://nasional.tempo.co/read/1601740/sekjen-...   \n",
       "\n",
       "                                          Relevant_urls  \\\n",
       "0     ['https://www.liputan6.com/cek-fakta/read/4121...   \n",
       "1     ['https://www.medcom.id/telusur/cek-fakta/ObzM...   \n",
       "2     ['https://turnbackhoax.id/2023/07/26/salah-pdi...   \n",
       "3     ['https://turnbackhoax.id/2023/01/16/salah-ind...   \n",
       "4     ['https://www.medcom.id/telusur/cek-fakta/nN94...   \n",
       "...                                                 ...   \n",
       "6236  ['https://nasional.kompas.com/read/2022/05/22/...   \n",
       "6237  ['https://www.kompas.com/global/read/2022/06/2...   \n",
       "6238  ['https://nasional.kompas.com/read/2018/05/02/...   \n",
       "6239  ['https://www.beritasatu.com/nasional/889697/s...   \n",
       "6240  ['https://nasional.tempo.co/read/1601740/sekje...   \n",
       "\n",
       "                                      Article_inspected  \\\n",
       "0      Liputan6.com, Jakarta - Restoran Padang Seder...   \n",
       "1      Beredar sebuah video melalui pesan berantai W...   \n",
       "2      Hasil periksa fakta Ummul Hidayah. Unggahan v...   \n",
       "3      Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...   \n",
       "4      Kanal PENA ISTANA membagikan video itu pada 1...   \n",
       "...                                                 ...   \n",
       "6236   JAKARTA, KOMPAS.com - Sejumlah menteri dalam ...   \n",
       "6237   YERUSALEM, KOMPAS.com - Parlemen Israel diper...   \n",
       "6238   JAKARTA, KOMPAS.com - Ketua umum Partai Gerin...   \n",
       "6239   Survei KedaiKopi: Elektabilitas Prabowo Terti...   \n",
       "6240   TEMPO.CO, Jakarta - Sekretaris Jenderal PDIP ...   \n",
       "\n",
       "                                         LLM_key_points         Tone  \\\n",
       "0     - Restoran Padang Sederhana dikabarkan membagi...      NEUTRAL   \n",
       "1     - Video yang beredar mengklaim memperlihatkan ...  NOT-NEUTRAL   \n",
       "2     - Unggahan video dengan klaim \"PDIP PANIK TOTA...  NOT-NEUTRAL   \n",
       "3     - Video yang mengklaim bahwa terjadi perang an...  NOT-NEUTRAL   \n",
       "4     - Video yang menyatakan bahwa Presiden Jokowi ...  NOT-NEUTRAL   \n",
       "...                                                 ...          ...   \n",
       "6236  - Sejumlah menteri dalam pemerintahan Presiden...      NEUTRAL   \n",
       "6237  - Parlemen Israel diperkirakan akan bubar pada...  NOT-NEUTRAL   \n",
       "6238  - Prabowo Subianto, Ketua Umum Partai Gerindra...      NEUTRAL   \n",
       "6239  - Lembaga survei KedaiKopi merilis hasil surve...  NOT-NEUTRAL   \n",
       "6240  - Sekretaris Jenderal PDIP, Hasto Kristiyanto,...      NEUTRAL   \n",
       "\n",
       "                       Topic  \\\n",
       "0                     SOSIAL   \n",
       "1                    LAINNYA   \n",
       "2                    POLITIK   \n",
       "3     HUKUM DAN KRIMINALITAS   \n",
       "4                    POLITIK   \n",
       "...                      ...   \n",
       "6236                 POLITIK   \n",
       "6237                 POLITIK   \n",
       "6238                 POLITIK   \n",
       "6239                 POLITIK   \n",
       "6240                 POLITIK   \n",
       "\n",
       "                                                      x  \n",
       "0     [Terdapat kabar di media sosial yang menyebutk...  \n",
       "1                                                  None  \n",
       "2                                                  None  \n",
       "3                                                  None  \n",
       "4                                                  None  \n",
       "...                                                 ...  \n",
       "6236                                               None  \n",
       "6237                                               None  \n",
       "6238                                               None  \n",
       "6239                                               None  \n",
       "6240                                               None  \n",
       "\n",
       "[6241 rows x 14 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at[0, 'x'] = b\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fa52b6a-dedf-43ed-b8b8-bd1f6e429e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 03:07:59.902944: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 03:07:59.913165: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-01 03:08:00.022414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 03:08:01.098193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from text_to_amr import TextToAMRSan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0ccd072-f7e4-4940-bbea-ba54f1911b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_amr = TextToAMRSan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf005f5-18ea-462d-a74b-ff9755ee649b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### data augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b787586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AMRFactGenerator import AMRFactDynamicGenerator\n",
    "from penman.models.amr import model as amr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e72a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_generator = AMRFactDynamicGenerator(model=amr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216c2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428636bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_augmented = df_train_concat_amr2.copy()\n",
    "df_augmented = df_train_concat_amr3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b0cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented['generated'] = None\n",
    "df_augmented['augmented'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171158f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fallback function for creating simple errors\n",
    "def create_fallback_error(amr_text, error_type):\n",
    "    \"\"\"Create a simpler fallback error by text manipulation when graph manipulation fails\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if error_type == \"predicate\":\n",
    "        # Find and modify a predicate by string replacement\n",
    "        predicates = re.findall(r'/ ([a-z]+-\\d+)', amr_text)\n",
    "        if predicates:\n",
    "            old_pred = predicates[0]\n",
    "            new_pred = old_pred.replace('-01', '-02')  # Change sense number\n",
    "            if old_pred == new_pred:  # If no change, try another approach\n",
    "                new_pred = \"alternative-01\"\n",
    "            modified_amr = amr_text.replace(f\"/ {old_pred}\", f\"/ {new_pred}\", 1)\n",
    "            return modified_amr, f\"Changed predicate from {old_pred} to {new_pred}\"\n",
    "    \n",
    "    elif error_type == \"entity\":\n",
    "        # Find and modify an entity name by string replacement\n",
    "        names = re.findall(r':op\\d+ \"([^\"]+)\"', amr_text)\n",
    "        if names:\n",
    "            old_name = names[0]\n",
    "            new_name = old_name[::-1]  # Reverse the name as a simple change\n",
    "            modified_amr = amr_text.replace(f':op1 \"{old_name}\"', f':op1 \"{new_name}\"', 1)\n",
    "            return modified_amr, f\"Changed entity name from {old_name} to {new_name}\"\n",
    "    \n",
    "    elif error_type == \"circumstance\":\n",
    "        # Add a simple time circumstance\n",
    "        if \":time\" not in amr_text:\n",
    "            # Add before the last closing parenthesis\n",
    "            modified_amr = amr_text.rstrip(')') + \"\\n    :time \\\"yesterday\\\")\"\n",
    "            return modified_amr, \"Added time circumstance 'yesterday'\"\n",
    "    \n",
    "    elif error_type == \"discourse\":\n",
    "        # Add a simple cause relation if not already present\n",
    "        if \":cause\" not in amr_text:\n",
    "            # Add before the last closing parenthesis\n",
    "            modified_amr = amr_text.rstrip(')') + \"\\n    :cause \\\"unknown\\\")\"\n",
    "            return modified_amr, \"Added cause relation 'unknown'\"\n",
    "    \n",
    "    elif error_type == \"out_of_article\":\n",
    "        # Add a simple comment that's clearly out of article\n",
    "        modified_amr = amr_text.rstrip(')') + \"\\n    :comment \\\"This is out of article information\\\")\"\n",
    "        return modified_amr, \"Added out of article comment\"\n",
    "    \n",
    "    # If all else fails, just return the original\n",
    "    return amr_text, \"No modification (fallback failed)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b881c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e9c38255e0478a8d07c652a00c4690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG3', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG3', 'z7')\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "Missing concept: ( z0 / mulai-01 :ARG0 ( z1 / orang :wiki \"Luis_Buñuel\" :name ( z2 / nama :op1 \"Bunuel\" ) ) :ARG1 ( z3 / sumbang-01 :ARG0 z1 :ARG1 ( z4 / artikel ) :ARG2 ( z5 / film :mod ( z6 / berbagai ) :mod ( z7 / periodik ) :mod ( z8 / terutama ) ) ) :mod ( z9 / juga ) :time ( z10 / asah-01 :ARG0 z1 :ARG1 ( z11 / keterampilan :poss z1 ) :ARG2 ( z12 / orang :ARG0-of ( z13 / punya-peran-org-91 :ARG2 ( z14 / ) ) ) ) ) \n",
      "ignoring secondary node contexts for 'z11'\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', '\"14:30\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', '\"14:30\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "cannot deinvert attribute: ('z5', ':part-of', 'z3')\n",
      "cannot deinvert attribute: ('z5', ':part-of', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z2')\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "Missing concept: ( z0 / perlu-01 :ARG0 ( z1 / kamu ) :ARG1 ( z2 / kembali-01 :ARG1 ( z3 / ambil-01 :ARG0 z1 :ARG1 ( z4 / debat-01 :mod ( z5 / ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z4', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':ARG1', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "Building failure\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 157, in decode_amr\n",
      "    graph = self._fix_and_make_graph(nodes)\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 442, in _fix_and_make_graph\n",
      "    graph = penman.decode(linearized + ' ')\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 191, in _decode\n",
      "    return codec.decode(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 56, in decode\n",
      "    tree = parse(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 34, in parse\n",
      "    return _parse(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 81, in _parse\n",
      "    node = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 163, in _parse_edge\n",
      "    raise tokens.error('Expected: SYMBOL, STRING, LPAREN', token=_next)\n",
      "penman.exceptions.DecodeError: \n",
      "  line 1\n",
      "    ( z0 / kontras-01 :ARG1 ( z1 / sebab-01 :ARG0 ( z2 / aku ) :ARG1 ( z3 / jadi-01 :ARG1 ( z4 / orang :mod ( z5 / homoseksual ) ) :ARG2 ( z6 / sehat-01 :polarity - :ARG1 z4 ) ) ) :ARG2 ( z7 / lihat-01 :polarity - :ARG0 z2 :ARG1 ( z8 / sesuatu :ARG1-of ( z9 / respon-01 ) ) :mod ( z10 / masih ) :topic ( z11 / sesuatu :ARG1-of ( z12 / kirim-01 :ARG0 ( z13 / orang :quant #267 ) ) ) ) ) \n",
      "                                                                                                                                                                                                                                                                                                                                                                                   ^\n",
      "DecodeError: Expected: SYMBOL, STRING, LPAREN\n",
      "\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':wiki', '\"gerakan-politik\"')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / peristiwa :mod ( z2 / orang :wiki \"George_W._Bush\" :name ( z3 / nama :op1 \"Bush\" ) ) ) :op2 ( z4 / peristiwa :ARG1-of ( z5 / ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z1', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z32', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z32', ':ARG2', '-')\n",
      "ignoring secondary node contexts for 'z8'\n",
      "ignoring epigraph data for duplicate triple: ('z14', ':time', 'z1003')\n",
      "ignoring epigraph data for duplicate triple: ('z14', ':time', 'z1003')\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':wiki', '\"gerakan-politik\"')\n",
      "Missing concept: ( z0 / usia-01 :ARG1 ( z1 / kamu ) :ARG2 ( z2 / amr-tidak-diketahui ) :mod ( z3 / ) ) \n",
      "Missing concept: ( z0 / multikalimat :snt1 ( z1 / tetap-01 :ARG1 ( z2 / entitas-persentase :value 14 ) :mod ( z3 / masih ) ) :snt2 ( z4 / tambah-01 :ARG2 ( z5 / ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z2')\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z6')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z6')\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z7')\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring epigraph data for duplicate triple: ('z20', ':ARG0', 'z12')\n",
      "ignoring epigraph data for duplicate triple: ('z20', ':ARG0', 'z12')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG0', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':day', '25')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':day', '25')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / tarik-01 :mode imperatif :ARG0 ( z2 / kamu ) :ARG2 ( z3 / atas :op1 ( z4 / kepala :part-of z2 ) ) ) :op2 ( z5 / segel-01 :mode imperatif :ARG0 z2 :ARG1-of ( z6 / erat-01 ) ) :op3 ( z7 / ambil-01 :mode imperatif :ARG0 z2 :ARG1 ( z8 / nap :quant ( z9 / banyak ) ) :mod ( z10 / dalam-dalam ) ) :op4 ( z11 / ikut-01 :mode imperatif :ARG0 ( z12 / kamu ) :ARG1 ( z13 / jalan :mod ( z14 / ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG1', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG1', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG4', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG4', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z3')\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':year', '11')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':year', '11')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':domain', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':domain', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':ARG0', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':ARG0', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':ARG1', 'z1071')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':instance', None)\n",
      "Building failure\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 157, in decode_amr\n",
      "    graph = self._fix_and_make_graph(nodes)\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 489, in _fix_and_make_graph\n",
      "    g = penman.decode(linearized)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 191, in _decode\n",
      "    return codec.decode(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 56, in decode\n",
      "    tree = parse(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 34, in parse\n",
      "    return _parse(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 81, in _parse\n",
      "    node = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 145, in _parse_edge\n",
      "    role_token = tokens.expect('ROLE')\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_lexer.py\", line 140, in expect\n",
      "    raise self.error(\n",
      "penman.exceptions.DecodeError: \n",
      "  line 151\n",
      "                                                                                                                     / thing\n",
      "                                                                                                                     ^\n",
      "DecodeError: Expected: ROLE\n",
      "\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':domain', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':domain', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z16', ':ARG0', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z16', ':ARG0', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring secondary node contexts for 'z11'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG2', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG2', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z72', ':mode', 'ekspresif-01')\n",
      "ignoring epigraph data for duplicate triple: ('z72', ':mode', 'ekspresif-01')\n",
      "Missing concept: ( z0 / Santai-01 :mode imperatif :ARG0 ( z1 / kamu ) :mod ( z2 / saja ) :ARG1-of ( z3 / sebab-01 :ARG0 ( z4 / punya-derajat-91 :ARG1 z1 :ARG2 ( z5 / kuat-01 :ARG1 z1 ) :ARG3 ( z6 / terlalu ) ) :condition ( z7 / laku-01 :ARG0 z1 :ARG1 ( z8 / sesuatu :ARG1-of ( z9 / serius-01 ) ) :time ( z10 / entitas-tanggal :weekday ( z11 / ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z13', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z13', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n"
     ]
    }
   ],
   "source": [
    "# Batch processing approach\n",
    "from AMRFactGenerator import AMRFactDynamicGenerator\n",
    "from penman.models.amr import model as amr_model\n",
    "import penman\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize error generator\n",
    "error_generator = AMRFactDynamicGenerator(model=amr_model)\n",
    "\n",
    "# Clone the dataframe\n",
    "df_augmented = df_train_concat_amr3.copy()\n",
    "\n",
    "# Add new columns\n",
    "df_augmented['generated'] = None\n",
    "df_augmented['augmented'] = None\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Process in batches\n",
    "total_batches = (len(df_augmented) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "results = []\n",
    "\n",
    "for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "    # Get batch slice\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df_augmented))\n",
    "    batch_indices = list(range(start_idx, end_idx))\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        send_telegram_message(f\"{batch_idx} processed\")\n",
    "    # Process each row in the batch\n",
    "    batch_results = []\n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            # Generate AMR from text\n",
    "            text = df_augmented.iloc[idx]['id']  # Use Indonesian text\n",
    "            amr_graph = text_to_amr(text)\n",
    "            amr_graph.metadata = {}  # Reset metadata\n",
    "            \n",
    "            # Store the generated AMR\n",
    "            try:\n",
    "                amr_text = penman.encode(amr_graph)\n",
    "                df_augmented.at[idx, 'generated'] = amr_text\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding generated AMR for row {idx}: {e}\")\n",
    "                batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "                continue\n",
    "            \n",
    "            # Generate all error types with fallbacks\n",
    "            augmentations = {}\n",
    "            for error_type in [\"predicate\", \"entity\", \"circumstance\", \"discourse\", \"out_of_article\"]:\n",
    "                try:\n",
    "                    # Generate error\n",
    "                    modified_graph, description = error_generator.introduce_error(copy.deepcopy(amr_graph), error_type)\n",
    "                    modified_amr = penman.encode(modified_graph)\n",
    "                    \n",
    "                    augmentations[error_type] = {\n",
    "                        'amr': modified_amr,\n",
    "                        'description': description\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    # Use fallback if needed\n",
    "                    try:\n",
    "                        fallback_amr, fallback_desc = create_fallback_error(amr_text, error_type)\n",
    "                        augmentations[error_type] = {\n",
    "                            'amr': fallback_amr,\n",
    "                            'description': f\"Fallback {error_type} error: {fallback_desc}\"\n",
    "                        }\n",
    "                    except:\n",
    "                        augmentations[error_type] = {\n",
    "                            'amr': amr_text,\n",
    "                            'description': f\"Error generating {error_type} error: {str(e)}\"\n",
    "                        }\n",
    "            \n",
    "            # Update the dataframe\n",
    "            df_augmented.at[idx, 'augmented'] = augmentations\n",
    "            batch_results.append({'idx': idx, 'status': 'success'})\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    # Add batch results to overall results\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Optional: Save checkpoint after each batch\n",
    "    if batch_idx % 5 == 0:\n",
    "        df_augmented.to_json(f'augmented_amr_checkpoint_{batch_idx}.json', orient='records')\n",
    "\n",
    "# Final save\n",
    "df_augmented.to_json('augmented_amr_dataset_3.0.json', orient='records')\n",
    "\n",
    "# Show stats\n",
    "print(f\"Successfully processed {sum(1 for r in results if r['status'] == 'success')} rows\")\n",
    "print(f\"Errors in {sum(1 for r in results if r['status'] == 'error')} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa026880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Predicate Error: Changed sense from 'utama-01' to 'utama-04'\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_augmented.iloc[0]['augmented']\n",
    "a['predicate']['amr']\n",
    "a['predicate']['description']\n",
    "# a['entity']['amr']\n",
    "# a['entity']['description']\n",
    "# a['circumstance']['amr']\n",
    "# a['circumstance']['description']\n",
    "# a['discourse']['amr']\n",
    "# a['discourse']['description']\n",
    "# a['out_of_article']['amr']\n",
    "# a['out_of_article']['description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f1d5-bdcf-407b-a1b5-2bc722b4341d",
   "metadata": {},
   "source": [
    "### data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb002ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07c3e61c0db431ca4bb859a8f041430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing concept: ( z0 / proses-01 :ARG1 ( z1 / mengembangkan-01 :ARG1 ( z2 / vaksin :wiki \"COVID-19\" :name ( z3 / nama :op1 \"COVID-19\" ) ) ) :ARG1-of ( z4 / melibatkan-01 :ARG2 ( z5 / vaksin :quant ( z6 / berbagai ) :ARG2-of ( z7 / golong-91 :ARG1 ( z8 / dan :op1 ( z9 / vaksin :mod ( z10 / virus :ARG1-of ( z11 / aktiv-01 ) ) ) :op2 ( z12 / vaksin :ARG1-of ( z13 / berbasis-01 :ARG2 ( z14 / protein ) ) ) :op3 ( z15 / vaksin :mod ( z16 / vektor ) ) :op4 ( z17 / vaksin :mod ( z18 / ) ) ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z8', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z8', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '\"telepon\"')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '\"telepon\"')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / tutup-01 :ARG1 ( z2 / foto ) :time ( z3 / telah ) ) :op2 ( z4 / tunjuk-01 :ARG0 ( z5 / gambar :mod ( z6 / asli ) ) :ARG1 ( z7 / laku-01 :ARG0 ( z8 / mereka ) :ARG1 ( z9 / telepon-01 :ARG0 z8 :ARG1 ( z10 / dan :op1 ( z11 / orang :wiki \"Tiongkok\" :name ( z12 / nama :op1 \"Joko\" :op2 \"Widodo\" ) :ARG0-of ( z13 / punya-peran-org-91 :ARG2 ( z14 / presiden ) ) ) :op2 ( z15 / orang :wiki \"Iriana\" :name ( z16 / nama :op1 \"Iriana\" ) :ARG0-of ( z17 / punya-peran-rel-91 :ARG1 z10 :ARG2 ( z18 / istri ) ) ) ) :manner ( z19 / video ) ) :time ( z20 / meraya-01 :ARG0 z8 :ARG1 ( z1000 / entitas-tanggal :month 1 :day 14 ) ) ) ) :ARG1-of ( z1001 / temu-01 :ARG0 ( z1002 / tim :mod ( z1003 / fakta ) :mod ( z1004 / ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z12', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z5', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z5', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':year', '20')\n",
      "Missing concept: ( z0 / tunjuk-01 :polarity - :ARG0 ( z1 / video :mod ( z2 / ini ) ) :ARG1 ( z3 / mata :part-of ( z4 / orang ) :ARG1-of ( z5 / benar-01 ) ) :ARG1-of ( z6 / sebab-01 :ARG0 ( z7 / punya-mod-91 :ARG1 z1 :ARG2 ( z8 / karya :mod ( z9 / seni :mod ( z10 / ) ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z11', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z11', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '26')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '26')\n"
     ]
    }
   ],
   "source": [
    "import penman\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Clone the dataframe\n",
    "df_amred = df_train_set_hki.copy()\n",
    "\n",
    "# Add new columns\n",
    "df_amred['amr_claim'] = None\n",
    "df_amred['amr_llm_key_points'] = None\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Process in batches\n",
    "total_batches = (len(df_amred) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "results = []\n",
    "\n",
    "for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "    # Get batch slice\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df_amred))\n",
    "    batch_indices = list(range(start_idx, end_idx))\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        send_telegram_message(f\"{batch_idx} processed\")\n",
    "    # Process each row in the batch\n",
    "    batch_results = []\n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            # Generate AMR from text\n",
    "            text = df_amred.iloc[idx]['Claim']  \n",
    "            amr_graph = text_to_amr(text)\n",
    "            amr_graph.metadata = {} \n",
    "\n",
    "            text = df_amred.iloc[idx]['LLM_key_points']\n",
    "            text_list = text.split(\"\\n- \")\n",
    "            text_list[0] = text_list[0].lstrip(\"- \")\n",
    "\n",
    "            amr_list = []\n",
    "            for a_text in text_list:\n",
    "                try:\n",
    "                    amr_graph = text_to_amr(a_text)\n",
    "                    amr_graph.metadata = {}\n",
    "                    amr_text = penman.encode(amr_graph)\n",
    "                    amr_list.append(amr_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"error encoding {e} for {a_text}\")\n",
    "            \n",
    "                \n",
    "            \n",
    "            # Store the generated AMR\n",
    "            try:\n",
    "                amr_text = penman.encode(amr_graph)\n",
    "                df_amred.at[idx, 'amr_claim'] = amr_text\n",
    "                df_amred.at[idx, 'amr_llm_key_points'] = amr_list\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding generated AMR for row {idx}: {e}\")\n",
    "                batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "                continue\n",
    "            \n",
    "            # Update the dataframe\n",
    "            batch_results.append({'idx': idx, 'status': 'success'})\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    # Add batch results to overall results\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Optional: Save checkpoint after each batch\n",
    "    if batch_idx % 5 == 0:\n",
    "        df_amred.to_json(f'IndoHoax_amr_checkpoint_{batch_idx}.json', orient='records')\n",
    "\n",
    "# Final save\n",
    "df_amred.to_json('indo_dax_amr.json', orient='records')\n",
    "send_telegram_message(f\"finish all process\")\n",
    "\n",
    "# Show stats\n",
    "print(f\"Successfully processed {sum(1 for r in results if r['status'] == 'success')} rows\")\n",
    "print(f\"Errors in {sum(1 for r in results if r['status'] == 'error')} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c332f8d-5ad7-4b11-b04a-f20cd7d697c4",
   "metadata": {},
   "source": [
    "# indosum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce4a8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../dataset/indosum/',\n",
       " ['devdataset', 'devdataset_amr', 'traindataset', 'testdataset'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_indosum, indosum_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae4759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cache-f2019a2ce00a50ea.arrow',\n",
       " 'cache-1ebe3459477a4b47.arrow',\n",
       " 'data-00000-of-00001.arrow',\n",
       " 'cache-2042b4b1d3cec83e.arrow',\n",
       " 'state.json',\n",
       " 'dataset_info.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_indosum+indosum_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e1c5b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  \\\n",
      "0  - Aliansi Organisasi Massa ( Ormas ) Islam se ...   \n",
      "1  Pelatih Persela Lamongan , Herry Kiswanto tak ...   \n",
      "2  - Direktorat Jenderal Pajak Kementerian Keuang...   \n",
      "3  - Sekretaris Jenderal Partai Komunis Vietnam N...   \n",
      "4  received by Romo Resi Brotonirmoyo ” yang dige...   \n",
      "\n",
      "                                             summary  \n",
      "0  Aliansi Organisasi Massa ( Ormas ) Islam se - ...  \n",
      "1  Pelatih Persela Lamongan , Herry Kiswanto tak ...  \n",
      "2  Direktorat Jenderal Pajak Kementerian Keuangan...  \n",
      "3  Sekretaris Jenderal Partai Komunis Vietnam Ngu...  \n",
      "4  Pertunjukkan wayang Indonesia bertema \" The Re...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "file_path = path_indosum + indosum_files[2]\n",
    "dataset = load_from_disk(file_path)\n",
    "\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca82a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Aliansi Organisasi Massa ( Ormas ) Islam se - Jabodetabek menyatakan siap menggelar aksi hari ini untuk menggugat Peraturan Pemerintah Pengganti Undang-Undang ( Perppu ) No.2 tahun 2017 tentang Organisasi Kemasyarakatan ( Ormas ) . \" Insya Allah jadi pukul 13.00 WIB di Monas , patung kuda , \" kata Koordinator Aksi Abu Zidan lewat pesan singkat , Selasa ( 18 / 7 ) . Berdasarkan selebaran poster elektronik beredar yang juga diterima CNNIndonesia.com , setidaknya ada 27 organisasi yang akan ikut beraksi . Mereka akan berkumpul bersama untuk menyampaikan pendapat menolak Perppu Ormas yang membuka peluang pemerintah membubarkan Ormas tersebut . Abu menjelaskan aksi ini bukan hanya dilakukan oleh Hizbut Tahrir Indonesia ( HTI ) . Jabodetabek . Di antara 27 ormas tersebut di antaranya ada Badan Koordinasi Lembaga Dakwah Kampus ( BKLDK ) , Pesantren mahasiswa UI , Ma'had UIN , Pelajar Islam Indonesia ( PII ) , dan Forum Mubaligh Bekasi . 00 WIB . 45 WIB . \" Kemungkinan salat Ashar tidak di lokasi aksi , tapi di masjid-masjid terdekat , \" kata Abu . Abu mengaku sudah mendapat izin dari kepolisian sejak Jum'at ( 14 / 7 ) lalu . Ia juga selalu berkomunikasi dengan kepolisian untuk teknis pelaksanaan aksi .\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0392e53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71353"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c385bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amr_article'] = None\n",
    "df['amr_summary'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "969c2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building failure\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 157, in decode_amr\n",
      "    graph = self._fix_and_make_graph(nodes)\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 489, in _fix_and_make_graph\n",
      "    g = penman.decode(linearized)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 191, in _decode\n",
      "    return codec.decode(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 56, in decode\n",
      "    tree = parse(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 34, in parse\n",
      "    return _parse(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 81, in _parse\n",
      "    node = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 145, in _parse_edge\n",
      "    role_token = tokens.expect('ROLE')\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_lexer.py\", line 140, in expect\n",
      "    raise self.error(\n",
      "penman.exceptions.DecodeError: \n",
      "  line 59\n",
      "                                                                   / thing\n",
      "                                                                   ^\n",
      "DecodeError: Expected: ROLE\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m summary \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m amr \u001b[38;5;241m=\u001b[39m text_to_amr(article)\n\u001b[0;32m----> 6\u001b[0m amr_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_amr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m amr\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m amr_summary\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/thesis/text_to_amr.py:54\u001b[0m, in \u001b[0;36mTextToAMRSan.__call__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     47\u001b[0m txt_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     48\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: txt_ids},\n\u001b[1;32m     49\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_gen_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamr_bos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamr_eos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Decode AMR\u001b[39;00m\n\u001b[1;32m     67\u001b[0m pred \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2286\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2279\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2280\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2281\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2286\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2297\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2299\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2300\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2306\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2307\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3506\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3503\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[0;32m-> 3506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3509\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3510\u001b[0m     outputs,\n\u001b[1;32m   3511\u001b[0m     model_kwargs,\n\u001b[1;32m   3512\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3513\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py:1609\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1607\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m-> 1609\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1626\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1628\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py:1495\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1489\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1490\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1491\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1492\u001b[0m     )\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1495\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py:1354\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1341\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1342\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1343\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1351\u001b[0m         use_cache,\n\u001b[1;32m   1352\u001b[0m     )\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py:683\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    682\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py:515\u001b[0m, in \u001b[0;36mMBartSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 515\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    article = df.iloc[i]['article']\n",
    "    summary = df.iloc[i]['summary']\n",
    "\n",
    "    amr = text_to_amr(article)\n",
    "    amr_summary = text_to_amr(summary)\n",
    "\n",
    "    amr.metadata = {}\n",
    "    amr_summary.metadata = {}\n",
    "\n",
    "    df.at[i, 'amr_article'] = penman.encode(amr)\n",
    "    df.at[i, 'amr_summary'] = penman.encode(amr_summary)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        df.to_csv(path_indosum + indosum_files[2] + \"/indosum_amr.csv\", index=False)\n",
    "        send_telegram_message(f\"{i} processed\")\n",
    "\n",
    "\n",
    "df.to_csv(path_indosum + indosum_files[2] + \"/indosum_amr.csv\", index=False)\n",
    "send_telegram_message(f\"finish all process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f0a5f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>amr_article</th>\n",
       "      <th>amr_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- Aliansi Organisasi Massa ( Ormas ) Islam se ...</td>\n",
       "      <td>Aliansi Organisasi Massa ( Ormas ) Islam se - ...</td>\n",
       "      <td># ::id 0\\n# ::annotator TextToAMRSan\\n# ::snt ...</td>\n",
       "      <td># ::id 0\\n# ::annotator TextToAMRSan\\n# ::snt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pelatih Persela Lamongan , Herry Kiswanto tak ...</td>\n",
       "      <td>Pelatih Persela Lamongan , Herry Kiswanto tak ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Direktorat Jenderal Pajak Kementerian Keuang...</td>\n",
       "      <td>Direktorat Jenderal Pajak Kementerian Keuangan...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Sekretaris Jenderal Partai Komunis Vietnam N...</td>\n",
       "      <td>Sekretaris Jenderal Partai Komunis Vietnam Ngu...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>received by Romo Resi Brotonirmoyo ” yang dige...</td>\n",
       "      <td>Pertunjukkan wayang Indonesia bertema \" The Re...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71348</th>\n",
       "      <td>Direktorat Reserse Kriminal Khususnya Polda Ri...</td>\n",
       "      <td>Direktorat Reserse Kriminal Khususnya Polda Ri...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71349</th>\n",
       "      <td>Kementerian Perdagangan telah menetapkan Harga...</td>\n",
       "      <td>Kementerian Perdagangan telah menetapkan Harga...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71350</th>\n",
       "      <td>Menteri Perhubungan Budi Karya Sumadi akan mem...</td>\n",
       "      <td>Menteri Perhubungan Budi Karya Sumadi akan mem...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71351</th>\n",
       "      <td>Duel antara Floyd Mayweather dan Conor McGrego...</td>\n",
       "      <td>Sejumlah persipan jelang duel antara Floyd May...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71352</th>\n",
       "      <td>Kepala Balai Taman Nasional Tambora Budi Kurni...</td>\n",
       "      <td>Kepala Balai Taman Nasional Tambora Budi Kurni...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71353 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      - Aliansi Organisasi Massa ( Ormas ) Islam se ...   \n",
       "1      Pelatih Persela Lamongan , Herry Kiswanto tak ...   \n",
       "2      - Direktorat Jenderal Pajak Kementerian Keuang...   \n",
       "3      - Sekretaris Jenderal Partai Komunis Vietnam N...   \n",
       "4      received by Romo Resi Brotonirmoyo ” yang dige...   \n",
       "...                                                  ...   \n",
       "71348  Direktorat Reserse Kriminal Khususnya Polda Ri...   \n",
       "71349  Kementerian Perdagangan telah menetapkan Harga...   \n",
       "71350  Menteri Perhubungan Budi Karya Sumadi akan mem...   \n",
       "71351  Duel antara Floyd Mayweather dan Conor McGrego...   \n",
       "71352  Kepala Balai Taman Nasional Tambora Budi Kurni...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0      Aliansi Organisasi Massa ( Ormas ) Islam se - ...   \n",
       "1      Pelatih Persela Lamongan , Herry Kiswanto tak ...   \n",
       "2      Direktorat Jenderal Pajak Kementerian Keuangan...   \n",
       "3      Sekretaris Jenderal Partai Komunis Vietnam Ngu...   \n",
       "4      Pertunjukkan wayang Indonesia bertema \" The Re...   \n",
       "...                                                  ...   \n",
       "71348  Direktorat Reserse Kriminal Khususnya Polda Ri...   \n",
       "71349  Kementerian Perdagangan telah menetapkan Harga...   \n",
       "71350  Menteri Perhubungan Budi Karya Sumadi akan mem...   \n",
       "71351  Sejumlah persipan jelang duel antara Floyd May...   \n",
       "71352  Kepala Balai Taman Nasional Tambora Budi Kurni...   \n",
       "\n",
       "                                             amr_article  \\\n",
       "0      # ::id 0\\n# ::annotator TextToAMRSan\\n# ::snt ...   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "71348                                               None   \n",
       "71349                                               None   \n",
       "71350                                               None   \n",
       "71351                                               None   \n",
       "71352                                               None   \n",
       "\n",
       "                                             amr_summary  \n",
       "0      # ::id 0\\n# ::annotator TextToAMRSan\\n# ::snt ...  \n",
       "1                                                   None  \n",
       "2                                                   None  \n",
       "3                                                   None  \n",
       "4                                                   None  \n",
       "...                                                  ...  \n",
       "71348                                               None  \n",
       "71349                                               None  \n",
       "71350                                               None  \n",
       "71351                                               None  \n",
       "71352                                               None  \n",
       "\n",
       "[71353 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49cb5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(z0 / kata-01\n",
      "    :ARG0 (z1 / orang\n",
      "              :wiki -\n",
      "              :name (z2 / nama\n",
      "                        :op1 \"Abu\"\n",
      "                        :op2 \"Zidan\")\n",
      "              :ARG0-of (z3 / koordinat-01\n",
      "                           :ARG1 (z4 / aksi)))\n",
      "    :ARG1 (z5 / siap-01\n",
      "              :ARG0 (z6 / organisasi\n",
      "                        :mod (z7 / gerakan-politik\n",
      "                                 :wiki -\n",
      "                                 :name (z8 / nama\n",
      "                                           :op1 \"Massa\"\n",
      "                                           :op2 \"Ormas\"))\n",
      "                        :location (z9 / kota\n",
      "                                      :wiki \"Jabodetabek\"\n",
      "                                      :name (z10 / nama\n",
      "                                                 :op1 \"Jabodetabek\")))\n",
      "              :ARG2 (z11 / laku-01\n",
      "                         :ARG0 z6\n",
      "                         :ARG1 (z12 / potong-01\n",
      "                                    :ARG0 z6\n",
      "                                    :ARG1 (z13 / organisasi))\n",
      "                         :time (z14 / hari\n",
      "                                    :mod (z15 / ini))\n",
      "                         :ARG1-of (z16 / lepas-91\n",
      "                                       :ARG2 (z17 / dan\n",
      "                                                  :op1 (z18 / entitas-tanggal\n",
      "                                                            :time \"0:00\"\n",
      "                                                            :day 18\n",
      "                                                            :month 7)\n",
      "                                                  :op2 (z19 / entitas-tanggal\n",
      "                                                            :time \"14:30\"))))))\n"
     ]
    }
   ],
   "source": [
    "import penman\n",
    "amr.metadata = {}\n",
    "print(penman.encode(amr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6d1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
