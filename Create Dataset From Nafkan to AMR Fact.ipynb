{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fba88f-5f08-4a0f-973c-fe02fa1ecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BOT_TOKEN = '6731314774:AAFNdgII0MX5Kdohl4aUq8cz9ArEsvdIdWw'\n",
    "CHAT_ID = '1653491203' \n",
    "\n",
    "def send_telegram_message(message):\n",
    "    url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'\n",
    "    data = {\n",
    "        'chat_id': CHAT_ID,\n",
    "        'text': message\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, data=data)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error sending message: {e}\")\n",
    "\n",
    "send_telegram_message(\"bot ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c618927c-da4d-44a2-8ac4-33bf2d9b3a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " '.ipynb_checkpoints',\n",
       " 'models',\n",
       " 'AMRAugmenter.py',\n",
       " '__pycache__',\n",
       " 'data_interface',\n",
       " 'amr_to_text_taufiq.py',\n",
       " '.git',\n",
       " 'amr_to_text_taufiq.ipynb',\n",
       " 'Create Dataset From Nafkan to AMR Fact.ipynb',\n",
       " '.gitignore',\n",
       " 'AMRFactGenerator.py',\n",
       " 'TextToAMR_sanVersion.ipynb',\n",
       " 'text_to_amr.py',\n",
       " 'model_interface',\n",
       " 'common',\n",
       " 'utils']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc4a51e-74f7-4158-9f30-a6d8466628a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_set_hki.csv', 'test_set_hki.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_dir = \"thesis\"\n",
    "\n",
    "# os.chdir(new_dir)\n",
    "\n",
    "os.listdir(\"../dataset/Dataset Berita Palsu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8004443f-6d63-4d4d-b674-152e9ce92903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import penman\n",
    "from utils.pointer_to_penman import convert_amr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48707d4-f3a5-421d-aa5c-b9fd2df159f9",
   "metadata": {},
   "source": [
    "# dataset nafkhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ba6290-af99-45cb-b911-e9e92df4781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_concat_amr2 = \"../dataset/ds/amrbart-id-concat-train-amr2/train.jsonl\"\n",
    "path_train_concat_amr3 = \"../dataset/ds/amrbart-id-concat-train-amr3/train.jsonl\"\n",
    "path_train_set_hki = \"../dataset/Dataset Berita Palsu/train_set_hki.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2369933-7fc7-4b64-beae-e1e64e7fae86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Content</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Source_Claim</th>\n",
       "      <th>Source_link</th>\n",
       "      <th>Relevant_urls</th>\n",
       "      <th>Article_inspected</th>\n",
       "      <th>LLM_key_points</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8343</td>\n",
       "      <td>fake</td>\n",
       "      <td>28/11/2019</td>\n",
       "      <td>Restoran Sederhana Ulang Tahun Ke-15 Membagika...</td>\n",
       "      <td>Beredar melalui Whatsapp pesan yang berisikan ...</td>\n",
       "      <td>Fabricated Content</td>\n",
       "      <td>whatsapp.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://www.liputan6.com/cek-fakta/read/4121...</td>\n",
       "      <td>Liputan6.com, Jakarta - Restoran Padang Seder...</td>\n",
       "      <td>- Restoran Padang Sederhana dikabarkan membagi...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>SOSIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10567</td>\n",
       "      <td>fake</td>\n",
       "      <td>17/11/2020</td>\n",
       "      <td>Video Pemindahan Kubur Imam Samudera akibat Pr...</td>\n",
       "      <td>Beredar sebuah video oleh akun Youtube Lida Ch...</td>\n",
       "      <td>Misleading Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>https://archive.vn/ydSmW</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/ObzM...</td>\n",
       "      <td>Beredar sebuah video melalui pesan berantai W...</td>\n",
       "      <td>- Video yang beredar mengklaim memperlihatkan ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>LAINNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18146</td>\n",
       "      <td>fake</td>\n",
       "      <td>26/07/2023</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>https://archive.cob.web.id/archive/1689934479....</td>\n",
       "      <td>['https://turnbackhoax.id/2023/07/26/salah-pdi...</td>\n",
       "      <td>Hasil periksa fakta Ummul Hidayah. Unggahan v...</td>\n",
       "      <td>- Unggahan video dengan klaim \"PDIP PANIK TOTA...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443</td>\n",
       "      <td>fake</td>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>Indonesia dan Australia Perang, Terdengar Suar...</td>\n",
       "      <td>Bener ga sih Indonesia dan Australia bakal perang</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Tiktok</td>\n",
       "      <td>http://archive.cob.web.id/archive/1673546109.2...</td>\n",
       "      <td>['https://turnbackhoax.id/2023/01/16/salah-ind...</td>\n",
       "      <td>Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...</td>\n",
       "      <td>- Video yang mengklaim bahwa terjadi perang an...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>HUKUM DAN KRIMINALITAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12455</td>\n",
       "      <td>fake</td>\n",
       "      <td>23/08/2021</td>\n",
       "      <td>Jokowi Lepas Baju PDIP, Megawati Murka</td>\n",
       "      <td>JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>archive.vn/kW5nJ</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/nN94...</td>\n",
       "      <td>Kanal PENA ISTANA membagikan video itu pada 1...</td>\n",
       "      <td>- Video yang menyatakan bahwa Presiden Jokowi ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Label        Date                                              Claim  \\\n",
       "0   8343  fake  28/11/2019  Restoran Sederhana Ulang Tahun Ke-15 Membagika...   \n",
       "1  10567  fake  17/11/2020  Video Pemindahan Kubur Imam Samudera akibat Pr...   \n",
       "2  18146  fake  26/07/2023  PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...   \n",
       "3  16443  fake  16/01/2023  Indonesia dan Australia Perang, Terdengar Suar...   \n",
       "4  12455  fake  23/08/2021             Jokowi Lepas Baju PDIP, Megawati Murka   \n",
       "\n",
       "                                             Content       Classification  \\\n",
       "0  Beredar melalui Whatsapp pesan yang berisikan ...   Fabricated Content   \n",
       "1  Beredar sebuah video oleh akun Youtube Lida Ch...   Misleading Content   \n",
       "2  PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...  Manipulated Content   \n",
       "3  Bener ga sih Indonesia dan Australia bakal perang  Manipulated Content   \n",
       "4  JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...  Manipulated Content   \n",
       "\n",
       "   Source_Claim                                        Source_link  \\\n",
       "0  whatsapp.com                                                NaN   \n",
       "1   Youtube.com                           https://archive.vn/ydSmW   \n",
       "2       Youtube  https://archive.cob.web.id/archive/1689934479....   \n",
       "3        Tiktok  http://archive.cob.web.id/archive/1673546109.2...   \n",
       "4   Youtube.com                                   archive.vn/kW5nJ   \n",
       "\n",
       "                                       Relevant_urls  \\\n",
       "0  ['https://www.liputan6.com/cek-fakta/read/4121...   \n",
       "1  ['https://www.medcom.id/telusur/cek-fakta/ObzM...   \n",
       "2  ['https://turnbackhoax.id/2023/07/26/salah-pdi...   \n",
       "3  ['https://turnbackhoax.id/2023/01/16/salah-ind...   \n",
       "4  ['https://www.medcom.id/telusur/cek-fakta/nN94...   \n",
       "\n",
       "                                   Article_inspected  \\\n",
       "0   Liputan6.com, Jakarta - Restoran Padang Seder...   \n",
       "1   Beredar sebuah video melalui pesan berantai W...   \n",
       "2   Hasil periksa fakta Ummul Hidayah. Unggahan v...   \n",
       "3   Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...   \n",
       "4   Kanal PENA ISTANA membagikan video itu pada 1...   \n",
       "\n",
       "                                      LLM_key_points         Tone  \\\n",
       "0  - Restoran Padang Sederhana dikabarkan membagi...      NEUTRAL   \n",
       "1  - Video yang beredar mengklaim memperlihatkan ...  NOT-NEUTRAL   \n",
       "2  - Unggahan video dengan klaim \"PDIP PANIK TOTA...  NOT-NEUTRAL   \n",
       "3  - Video yang mengklaim bahwa terjadi perang an...  NOT-NEUTRAL   \n",
       "4  - Video yang menyatakan bahwa Presiden Jokowi ...  NOT-NEUTRAL   \n",
       "\n",
       "                    Topic  \n",
       "0                  SOSIAL  \n",
       "1                 LAINNYA  \n",
       "2                 POLITIK  \n",
       "3  HUKUM DAN KRIMINALITAS  \n",
       "4                 POLITIK  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# df_train_concat_amr2 = pd.read_json(path_train_concat_amr2, lines=True)\n",
    "# df_train_concat_amr2.head()\n",
    "# df_train_concat_amr3 = pd.read_json(path_train_concat_amr3, lines=True)\n",
    "# df_train_concat_amr3.head()\n",
    "df_train_set_hki = pd.read_csv(path_train_set_hki)\n",
    "df_train_set_hki.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10214ac2-dbbf-416f-8d42-13a1f82d581a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6241"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_set_hki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362bd21a-e020-4280-9eb0-5958925aba1a",
   "metadata": {},
   "source": [
    "# dataset indohoax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7491b828-4229-462c-bef7-f2c4da67bafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                               17483\n",
       "Label                                                             fake\n",
       "Date                                                        13/04/2023\n",
       "Claim                Pesulap Merah Meninggal Dunia Usai Adu Kesakti...\n",
       "Content              innalilahi wainnailaihi hirojiun\\nhari ini dik...\n",
       "Classification                                      Fabricated Content\n",
       "Source_Claim                                                   Youtube\n",
       "Source_link                                   https://archive.fo/limxu\n",
       "Relevant_urls        ['https://www.liputan6.com/showbiz/read/526133...\n",
       "Article_inspected     Liputan6.com, Jakarta - Media sosial tengah h...\n",
       "LLM_key_points       - Terdapat kabar di media sosial yang menyebut...\n",
       "Tone                                                       NOT-NEUTRAL\n",
       "Topic                                                          HIBURAN\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_train_set_hki.iloc[100]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4500da2-beb7-41b1-accd-e78a3daa2fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pesulap Merah Meninggal Dunia Usai Adu Kesaktian Dengan Dukun'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['Claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014687ca-fdee-487a-82f1-c4a53b66ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "innalilahi wainnailaihi hirojiun\n",
      "hari ini dikabarkan pesulap merah atau\n",
      "Marsel radival meninggal dunia seusai adu kesaktian dengan dukun\n"
     ]
    }
   ],
   "source": [
    "print(a['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68e9350c-ddde-42b8-bc29-c048da3ba3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Liputan6.com, Jakarta - Media sosial tengah heboh dengan pemberitaan Marcel Radhival alias Pesulap Merah. Ia disebut-sebut telah meninggal dunia.\n",
      "Dari narasi yang beredar, Pesulap Merah disebut meninggal dunia usai beradu kesaktian dengan seorang dukun. Sebagaimana diketahui, selama ini Pesulap Merah memang begitu lantang melawan kebohongan-kebohongan ilmu perdukunan.\n",
      "Marcel kemudian bereaksi dengan berita hoax yang ada. Di Instagram-nya, Pesulap Merah mengunggah sebuah tangkapan layar sebuah video dari kanal YouTube yang memberitakan dirinya meninggal dunia.\n",
      "Advertisement\n",
      "\"INNALILLAHI.. MARSEL RADIVAL MENINGGAL DUNIA USAI ADU KESAKTIAN DENGAN DUKUN,\" begitu judul yang tertulis dalam video di kanal TC Media yang diunggah ulang di Instagram Marcel beberapa waktu lalu.\n",
      "* Follow Official WhatsApp Channel Liputan6.com untuk mendapatkan berita-berita terkini dengan mengklik tautan ini.\n",
      "Banyak Konten Hoax Disebar di Grup-Grup\n",
      "Menyertai unggahan itu, Pesulap Merah menuliskan keterangan yang bernada meledek si berita hoax tersebut. Ia juga khawatir bahwa konten-konten seperti ini lah yang sangat mungkin bisa menyesatkan.\n",
      "\"Waaaah sekarang saya jadi hantu chuakakakssss terrrrrrrrr. Kalo dishare ke tiktok atau group keluarga pasti pada percaya nih sama berita sampah beginian,\" tulis Pesulap Merah.\n",
      "Advertisement\n",
      "Masyarakat Indonesia Minim Literasi\n",
      "Lebih lanjut, pesulap merah menyebut bahwa ini menjadi bukti bagaimana masyarakat Indonesia masih sangat minim literasi.\n",
      "\"bhahaha parah memang, dari kasus saya difitnah (lagi) ini, membuktikan banyak masyarakat indonesia yang MINIM LITERASI alias GAMPANG PERCAYAAN TERHADAP HAL YANG TIDAK ADA BUKTINYA. #PesulapMerah #ILMUMERAH,\" sambung Pesulap Merah lagi.\n",
      "Banyak Warganet yang Hampir Percaya\n",
      "Di kolom komentar, banyak juga warganet yang mengaku hampir percaya dengan konten-konten hoax tersebut.\n",
      "\"wallahi ana kaget, ana kira bener tau tau ente yg post. panjang umur insyaallah pesulap merah!\" tulis @ali.habsyi.\n",
      "\"Bang @marcelradhival1 gw debat Amal bapak gw gara\" dia nonton ytb soal lu yg meninggal. Gw udah bilang lu masih idup gw tunjukkin akun Instagram lu die keukeuh gk percaya,\" timpal @ladyrocker.nikeardilla.\n",
      "* Fakta atau Hoaks? Untuk mengetahui kebenaran informasi yang beredar, silakan WhatsApp ke nomor Cek Fakta Liputan6.com 0811 9787 670 hanya dengan ketik kata kunci yang diinginkan.\n",
      "AdvertisementCek Fakta: Benarkah Pesulap Merah Meninggal Dunia Usai Adu Kesaktian .... PIKIRAN RAKYAT - Beredar kabar di media sosial, yang menyebutkan Pesulap Merah atau pemilik nama Marcel Radhival meninggal dunia usai adu kekuatan dengan dukun. Tampak tangkapan layar YouTube dengan thumbnail menyebutkan jika Pesulap Merah kalah dari dukun hingga meninggal dunia. Cek Fakta: Pesulap Merah Meninggal Dunia Usai Adu Kesaktian dengan Dukun\n",
      "Dream - Laman media sosial, khususnya Youtube, geger dengan kabar Marcel Radhival atau yang dikenal dengan nama Pesulap Merah meninggal dunia. Informasi yang beredar menyebutkan pesulap merah itu meninggal usai adu kesaktian dengan seorang dukun.\n",
      "Informasi tersebut diketahui pertama kali beredar di channel YouTubeTC Media.\n",
      "\"INNALILLAHI.. MARSEL RADIVAL MENINGGAL DUNIA USAI ADU KESAKTIAN DENGAN DUKUN,\" demikian narasi postingan tersebut.\n",
      "-\n",
      "Rumus apa yang pertama kali ditunjukkan kepada Kenkulus dalam video viralnya? Seperti dalam salah satu video, Ken tampak dites dengan rumus-rumus. Sang ayah memperlihatkan sebuah rumus dalam kertas biru, lalu Ken menyebut \"piramid\". Ternyata benar, karena itu merupakan rumus untuk menghitung piramid.\n",
      "-\n",
      "Siapa yang memulai percakapan pertama di media sosial? Rupanya Maulana lah yang menyapa duluan dengan mengirimkan pesan menyapa Chan dengan sangat singkat. ‘heii,\" tulis Maulana kepada Chan.\n",
      "-\n",
      "Kapan video tersebut direkam? Momen yang terjadi pada Oktober 2015 silam itu pun kembali viral dan mendapat sorotan dari netizen yang melihatnya.\n",
      "Pada thumbnail video berdurasi empat menit tersebut, terlihat foto Pesulap Merah, serta pembawa acara berita dan juga potongan mobil ambulans yang diduga membawa jenazah Pesulap Merah.\n",
      "Dalam narasinya menerangkan, pesulap merah meninggal akibat beradu pembuktian ilmu santet di Jakarta, dengan pemuda asal suku pedalaman Kalimantan Tengah.\n",
      "Saat pembuktian ilmu Pesulap Merah biasa saja, namun saat kembali ke rumah ia merasa mual dan akhirnya meninggal saat dibawa ke rumah sakit.\n",
      "Lalu, benarkah kabar Pesulap Merah meninggal dunia karena adu kesaktian dengan dukun?\n",
      "Melalui akun instagram resminya @marcelradhival1, Pesulap Merah mengklarifikasi bahwa informasi yang disebarkan oleh kanal Youtube tersebut adalah hoaks.\n",
      "“Waaaah sekarang saya jadi hantu chuakakakssss terrrrrrrrr\n",
      "Kalo dishare ke tiktok atau group keluarga pasti pada percaya nih sama berita sampah beginian. bhahaha parah memang, dari kasus saya difitnah (lagi) ini, membuktikan banyak masyarakat indonesia yang MINIM LITERASI alias GAMPANG PERCAYAAN TERHADAP HAL YANG TIDAK ADA BUKTINYA”, tulisnya.\n",
      "Cobain For You Page (FYP) Yang kamu suka ada di sini,\n",
      "lihat isinya\n",
      "Wanita WNI ditemukan tewas dengan luka tusuk, suami dilaporkan bunuh diri.\n",
      "Baca SelengkapnyaIbu terduga pelaku pembunuhan anak kandungnya di Bekasi sempat tertawa saat diperiksa.\n",
      "Baca SelengkapnyaAwalnya banyak pemirsa, terutama yang dari Indonesia, menduga orangtua Upin dan Ipin meninggal dunia. Namun tidak diketahui penyebab mereka meninggal.\n",
      "Baca Selengkapnyavideo untuk kamu.\n",
      "Faktanya, konsep makam ini telah ada sejak lama. Yuk, simak makam-makam tertua yang ada di dunia!\n",
      "Baca SelengkapnyaOjol temukan kerangka manusia di lahan kosong di Makassar, diduga berjenis kelamin perempuan.\n",
      "Baca SelengkapnyaBulog telah menyelidiki video viral yang menunjukkan buruh berguling-guling sambil mandi beras.\n",
      "Baca SelengkapnyaKecelakaan tunggal di jurang kawasan Bromo, 4 orang meninggal dunia, 5 orang yang selamat dibawa ke rumah sakit.\n",
      "Baca SelengkapnyaSelalu menolak diajak bertemu, pemuda di Lamongan gagal nikah pasangan tak hadir saat hari H.\n",
      "Baca SelengkapnyaBeberapa orang mungkin mengalami gangguan kecemasan yang bisa semakin parah. Yuk, simak cara mengatasinya!\n",
      "Baca SelengkapnyaKalian saat memakai celana kepanjangan pasti suka melipat bagian bawahnya biar terlihat rapi. Nah, mungkin tips ini yang sedang kalian cari.\n",
      "Baca Selengkapnya\n"
     ]
    }
   ],
   "source": [
    "print(a['Article_inspected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40628d35-a3f3-496b-abee-a0bb6ad78d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Terdapat kabar di media sosial yang menyebutkan bahwa Marcel Radhival, yang dikenal sebagai Pesulap Merah, telah meninggal dunia setelah beradu kesaktian dengan seorang dukun.', 'Kabar tersebut pertama kali beredar di kanal YouTube TC Media dengan judul yang menyatakan bahwa Pesulap Merah meninggal dunia akibat adu kesaktian.', 'Pesulap Merah mengklarifikasi melalui akun Instagram-nya bahwa informasi tersebut adalah hoaks dan tidak benar.', 'Dalam unggahannya, Pesulap Merah menyatakan bahwa berita tersebut adalah \"berita sampah\" dan menunjukkan bahwa masyarakat Indonesia masih minim literasi, sehingga mudah percaya pada informasi yang tidak memiliki bukti.', 'Banyak warganet yang hampir percaya dengan berita hoax tersebut, menunjukkan reaksi kaget dan mengaku mendengar kabar tersebut dari orang lain.']\n"
     ]
    }
   ],
   "source": [
    "b = a['LLM_key_points'].split(\"\\n- \")\n",
    "b[0] = b[0].lstrip(\"- \")\n",
    "# b = a['LLM_key_points']\n",
    "print(b)\n",
    "# for i in b:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab78a2ee-6720-4393-9210-5de0a1b900b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_train_set_hki)\n",
    "df['x'] = None\n",
    "# df = pd.DataFrame({'x': [None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "120a3c57-4f57-4970-8440-6fa02cc23483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Date</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Content</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Source_Claim</th>\n",
       "      <th>Source_link</th>\n",
       "      <th>Relevant_urls</th>\n",
       "      <th>Article_inspected</th>\n",
       "      <th>LLM_key_points</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Topic</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8343</td>\n",
       "      <td>fake</td>\n",
       "      <td>28/11/2019</td>\n",
       "      <td>Restoran Sederhana Ulang Tahun Ke-15 Membagika...</td>\n",
       "      <td>Beredar melalui Whatsapp pesan yang berisikan ...</td>\n",
       "      <td>Fabricated Content</td>\n",
       "      <td>whatsapp.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://www.liputan6.com/cek-fakta/read/4121...</td>\n",
       "      <td>Liputan6.com, Jakarta - Restoran Padang Seder...</td>\n",
       "      <td>- Restoran Padang Sederhana dikabarkan membagi...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>SOSIAL</td>\n",
       "      <td>[Terdapat kabar di media sosial yang menyebutk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10567</td>\n",
       "      <td>fake</td>\n",
       "      <td>17/11/2020</td>\n",
       "      <td>Video Pemindahan Kubur Imam Samudera akibat Pr...</td>\n",
       "      <td>Beredar sebuah video oleh akun Youtube Lida Ch...</td>\n",
       "      <td>Misleading Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>https://archive.vn/ydSmW</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/ObzM...</td>\n",
       "      <td>Beredar sebuah video melalui pesan berantai W...</td>\n",
       "      <td>- Video yang beredar mengklaim memperlihatkan ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>LAINNYA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18146</td>\n",
       "      <td>fake</td>\n",
       "      <td>26/07/2023</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube</td>\n",
       "      <td>https://archive.cob.web.id/archive/1689934479....</td>\n",
       "      <td>['https://turnbackhoax.id/2023/07/26/salah-pdi...</td>\n",
       "      <td>Hasil periksa fakta Ummul Hidayah. Unggahan v...</td>\n",
       "      <td>- Unggahan video dengan klaim \"PDIP PANIK TOTA...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443</td>\n",
       "      <td>fake</td>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>Indonesia dan Australia Perang, Terdengar Suar...</td>\n",
       "      <td>Bener ga sih Indonesia dan Australia bakal perang</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Tiktok</td>\n",
       "      <td>http://archive.cob.web.id/archive/1673546109.2...</td>\n",
       "      <td>['https://turnbackhoax.id/2023/01/16/salah-ind...</td>\n",
       "      <td>Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...</td>\n",
       "      <td>- Video yang mengklaim bahwa terjadi perang an...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>HUKUM DAN KRIMINALITAS</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12455</td>\n",
       "      <td>fake</td>\n",
       "      <td>23/08/2021</td>\n",
       "      <td>Jokowi Lepas Baju PDIP, Megawati Murka</td>\n",
       "      <td>JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...</td>\n",
       "      <td>Manipulated Content</td>\n",
       "      <td>Youtube.com</td>\n",
       "      <td>archive.vn/kW5nJ</td>\n",
       "      <td>['https://www.medcom.id/telusur/cek-fakta/nN94...</td>\n",
       "      <td>Kanal PENA ISTANA membagikan video itu pada 1...</td>\n",
       "      <td>- Video yang menyatakan bahwa Presiden Jokowi ...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>3248</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Moral Politik Para Menteri Jelang 2024: Manuve...</td>\n",
       "      <td>Menyoal Moral Politik di Tengah Manuver Para M...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://nasional.kompas.com/read/2022/05/22/105...</td>\n",
       "      <td>['https://nasional.kompas.com/read/2022/05/22/...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Sejumlah menteri dalam ...</td>\n",
       "      <td>- Sejumlah menteri dalam pemerintahan Presiden...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>2374</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Krisis Politik Israel: Parlemen Bubar, Netanya...</td>\n",
       "      <td>Krisis Politik Israel: Parlemen Bubar, Bagaima...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://www.kompas.com/global/read/2022/06/29/1...</td>\n",
       "      <td>['https://www.kompas.com/global/read/2022/06/2...</td>\n",
       "      <td>YERUSALEM, KOMPAS.com - Parlemen Israel diper...</td>\n",
       "      <td>- Parlemen Israel diperkirakan akan bubar pada...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>3468</td>\n",
       "      <td>real</td>\n",
       "      <td>22/02/2023</td>\n",
       "      <td>Bawaslu: Kontrak Politik Prabowo dengan KSPI S...</td>\n",
       "      <td>Tanggapan Bawaslu Soal Kontrak Politik Prabowo...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>KOMPAS</td>\n",
       "      <td>http://nasional.kompas.com/read/2018/05/02/214...</td>\n",
       "      <td>['https://nasional.kompas.com/read/2018/05/02/...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Ketua umum Partai Gerin...</td>\n",
       "      <td>- Prabowo Subianto, Ketua Umum Partai Gerindra...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>271</td>\n",
       "      <td>real</td>\n",
       "      <td>24/12/2021</td>\n",
       "      <td>KedaiKOPI: Elektabilitas Prabowo Tertinggi Ber...</td>\n",
       "      <td>KedaiKOPI Sebut Elektabilitas Tinggi Prabowo K...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/20211224...</td>\n",
       "      <td>['https://www.beritasatu.com/nasional/889697/s...</td>\n",
       "      <td>Survei KedaiKopi: Elektabilitas Prabowo Terti...</td>\n",
       "      <td>- Lembaga survei KedaiKopi merilis hasil surve...</td>\n",
       "      <td>NOT-NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6240</th>\n",
       "      <td>4147</td>\n",
       "      <td>real</td>\n",
       "      <td>14/06/2022</td>\n",
       "      <td>Sekjen PDIP Hasto Kristiyanto: Jangan Terbuai ...</td>\n",
       "      <td>Sekjen PDIP Peringatkan Kader Partainya Tak Te...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>TEMPO</td>\n",
       "      <td>https://nasional.tempo.co/read/1601740/sekjen-...</td>\n",
       "      <td>['https://nasional.tempo.co/read/1601740/sekje...</td>\n",
       "      <td>TEMPO.CO, Jakarta - Sekretaris Jenderal PDIP ...</td>\n",
       "      <td>- Sekretaris Jenderal PDIP, Hasto Kristiyanto,...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>POLITIK</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6241 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Label        Date  \\\n",
       "0      8343  fake  28/11/2019   \n",
       "1     10567  fake  17/11/2020   \n",
       "2     18146  fake  26/07/2023   \n",
       "3     16443  fake  16/01/2023   \n",
       "4     12455  fake  23/08/2021   \n",
       "...     ...   ...         ...   \n",
       "6236   3248  real  22/02/2023   \n",
       "6237   2374  real  22/02/2023   \n",
       "6238   3468  real  22/02/2023   \n",
       "6239    271  real  24/12/2021   \n",
       "6240   4147  real  14/06/2022   \n",
       "\n",
       "                                                  Claim  \\\n",
       "0     Restoran Sederhana Ulang Tahun Ke-15 Membagika...   \n",
       "1     Video Pemindahan Kubur Imam Samudera akibat Pr...   \n",
       "2     PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...   \n",
       "3     Indonesia dan Australia Perang, Terdengar Suar...   \n",
       "4                Jokowi Lepas Baju PDIP, Megawati Murka   \n",
       "...                                                 ...   \n",
       "6236  Moral Politik Para Menteri Jelang 2024: Manuve...   \n",
       "6237  Krisis Politik Israel: Parlemen Bubar, Netanya...   \n",
       "6238  Bawaslu: Kontrak Politik Prabowo dengan KSPI S...   \n",
       "6239  KedaiKOPI: Elektabilitas Prabowo Tertinggi Ber...   \n",
       "6240  Sekjen PDIP Hasto Kristiyanto: Jangan Terbuai ...   \n",
       "\n",
       "                                                Content       Classification  \\\n",
       "0     Beredar melalui Whatsapp pesan yang berisikan ...   Fabricated Content   \n",
       "1     Beredar sebuah video oleh akun Youtube Lida Ch...   Misleading Content   \n",
       "2     PDIP PANIK TOTAL!! GANJAR PUAN KELUAR DARI DAF...  Manipulated Content   \n",
       "3     Bener ga sih Indonesia dan Australia bakal perang  Manipulated Content   \n",
       "4     JOKOWI LEPAS BAJU PDIP. MEGAWATI MARAH JOKOWI ...  Manipulated Content   \n",
       "...                                                 ...                  ...   \n",
       "6236  Menyoal Moral Politik di Tengah Manuver Para M...                 TRUE   \n",
       "6237  Krisis Politik Israel: Parlemen Bubar, Bagaima...                 TRUE   \n",
       "6238  Tanggapan Bawaslu Soal Kontrak Politik Prabowo...                 TRUE   \n",
       "6239  KedaiKOPI Sebut Elektabilitas Tinggi Prabowo K...                 TRUE   \n",
       "6240  Sekjen PDIP Peringatkan Kader Partainya Tak Te...                 TRUE   \n",
       "\n",
       "      Source_Claim                                        Source_link  \\\n",
       "0     whatsapp.com                                                NaN   \n",
       "1      Youtube.com                           https://archive.vn/ydSmW   \n",
       "2          Youtube  https://archive.cob.web.id/archive/1689934479....   \n",
       "3           Tiktok  http://archive.cob.web.id/archive/1673546109.2...   \n",
       "4      Youtube.com                                   archive.vn/kW5nJ   \n",
       "...            ...                                                ...   \n",
       "6236        KOMPAS  http://nasional.kompas.com/read/2022/05/22/105...   \n",
       "6237        KOMPAS  http://www.kompas.com/global/read/2022/06/29/1...   \n",
       "6238        KOMPAS  http://nasional.kompas.com/read/2018/05/02/214...   \n",
       "6239           CNN  https://www.cnnindonesia.com/nasional/20211224...   \n",
       "6240         TEMPO  https://nasional.tempo.co/read/1601740/sekjen-...   \n",
       "\n",
       "                                          Relevant_urls  \\\n",
       "0     ['https://www.liputan6.com/cek-fakta/read/4121...   \n",
       "1     ['https://www.medcom.id/telusur/cek-fakta/ObzM...   \n",
       "2     ['https://turnbackhoax.id/2023/07/26/salah-pdi...   \n",
       "3     ['https://turnbackhoax.id/2023/01/16/salah-ind...   \n",
       "4     ['https://www.medcom.id/telusur/cek-fakta/nN94...   \n",
       "...                                                 ...   \n",
       "6236  ['https://nasional.kompas.com/read/2022/05/22/...   \n",
       "6237  ['https://www.kompas.com/global/read/2022/06/2...   \n",
       "6238  ['https://nasional.kompas.com/read/2018/05/02/...   \n",
       "6239  ['https://www.beritasatu.com/nasional/889697/s...   \n",
       "6240  ['https://nasional.tempo.co/read/1601740/sekje...   \n",
       "\n",
       "                                      Article_inspected  \\\n",
       "0      Liputan6.com, Jakarta - Restoran Padang Seder...   \n",
       "1      Beredar sebuah video melalui pesan berantai W...   \n",
       "2      Hasil periksa fakta Ummul Hidayah. Unggahan v...   \n",
       "3      Hasil Periksa Fakta Dyah Febriyani\\nVideo ter...   \n",
       "4      Kanal PENA ISTANA membagikan video itu pada 1...   \n",
       "...                                                 ...   \n",
       "6236   JAKARTA, KOMPAS.com - Sejumlah menteri dalam ...   \n",
       "6237   YERUSALEM, KOMPAS.com - Parlemen Israel diper...   \n",
       "6238   JAKARTA, KOMPAS.com - Ketua umum Partai Gerin...   \n",
       "6239   Survei KedaiKopi: Elektabilitas Prabowo Terti...   \n",
       "6240   TEMPO.CO, Jakarta - Sekretaris Jenderal PDIP ...   \n",
       "\n",
       "                                         LLM_key_points         Tone  \\\n",
       "0     - Restoran Padang Sederhana dikabarkan membagi...      NEUTRAL   \n",
       "1     - Video yang beredar mengklaim memperlihatkan ...  NOT-NEUTRAL   \n",
       "2     - Unggahan video dengan klaim \"PDIP PANIK TOTA...  NOT-NEUTRAL   \n",
       "3     - Video yang mengklaim bahwa terjadi perang an...  NOT-NEUTRAL   \n",
       "4     - Video yang menyatakan bahwa Presiden Jokowi ...  NOT-NEUTRAL   \n",
       "...                                                 ...          ...   \n",
       "6236  - Sejumlah menteri dalam pemerintahan Presiden...      NEUTRAL   \n",
       "6237  - Parlemen Israel diperkirakan akan bubar pada...  NOT-NEUTRAL   \n",
       "6238  - Prabowo Subianto, Ketua Umum Partai Gerindra...      NEUTRAL   \n",
       "6239  - Lembaga survei KedaiKopi merilis hasil surve...  NOT-NEUTRAL   \n",
       "6240  - Sekretaris Jenderal PDIP, Hasto Kristiyanto,...      NEUTRAL   \n",
       "\n",
       "                       Topic  \\\n",
       "0                     SOSIAL   \n",
       "1                    LAINNYA   \n",
       "2                    POLITIK   \n",
       "3     HUKUM DAN KRIMINALITAS   \n",
       "4                    POLITIK   \n",
       "...                      ...   \n",
       "6236                 POLITIK   \n",
       "6237                 POLITIK   \n",
       "6238                 POLITIK   \n",
       "6239                 POLITIK   \n",
       "6240                 POLITIK   \n",
       "\n",
       "                                                      x  \n",
       "0     [Terdapat kabar di media sosial yang menyebutk...  \n",
       "1                                                  None  \n",
       "2                                                  None  \n",
       "3                                                  None  \n",
       "4                                                  None  \n",
       "...                                                 ...  \n",
       "6236                                               None  \n",
       "6237                                               None  \n",
       "6238                                               None  \n",
       "6239                                               None  \n",
       "6240                                               None  \n",
       "\n",
       "[6241 rows x 14 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.at[0, 'x'] = b\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fa52b6a-dedf-43ed-b8b8-bd1f6e429e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 02:22:21.805376: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 02:22:21.808157: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-28 02:22:21.865883: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 02:22:22.800563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from text_to_amr import TextToAMRSan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0ccd072-f7e4-4940-bbea-ba54f1911b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_amr = TextToAMRSan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf005f5-18ea-462d-a74b-ff9755ee649b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### data augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b787586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AMRFactGenerator import AMRFactDynamicGenerator\n",
    "from penman.models.amr import model as amr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e72a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_generator = AMRFactDynamicGenerator(model=amr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216c2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428636bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_augmented = df_train_concat_amr2.copy()\n",
    "df_augmented = df_train_concat_amr3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b0cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented['generated'] = None\n",
    "df_augmented['augmented'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171158f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fallback function for creating simple errors\n",
    "def create_fallback_error(amr_text, error_type):\n",
    "    \"\"\"Create a simpler fallback error by text manipulation when graph manipulation fails\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if error_type == \"predicate\":\n",
    "        # Find and modify a predicate by string replacement\n",
    "        predicates = re.findall(r'/ ([a-z]+-\\d+)', amr_text)\n",
    "        if predicates:\n",
    "            old_pred = predicates[0]\n",
    "            new_pred = old_pred.replace('-01', '-02')  # Change sense number\n",
    "            if old_pred == new_pred:  # If no change, try another approach\n",
    "                new_pred = \"alternative-01\"\n",
    "            modified_amr = amr_text.replace(f\"/ {old_pred}\", f\"/ {new_pred}\", 1)\n",
    "            return modified_amr, f\"Changed predicate from {old_pred} to {new_pred}\"\n",
    "    \n",
    "    elif error_type == \"entity\":\n",
    "        # Find and modify an entity name by string replacement\n",
    "        names = re.findall(r':op\\d+ \"([^\"]+)\"', amr_text)\n",
    "        if names:\n",
    "            old_name = names[0]\n",
    "            new_name = old_name[::-1]  # Reverse the name as a simple change\n",
    "            modified_amr = amr_text.replace(f':op1 \"{old_name}\"', f':op1 \"{new_name}\"', 1)\n",
    "            return modified_amr, f\"Changed entity name from {old_name} to {new_name}\"\n",
    "    \n",
    "    elif error_type == \"circumstance\":\n",
    "        # Add a simple time circumstance\n",
    "        if \":time\" not in amr_text:\n",
    "            # Add before the last closing parenthesis\n",
    "            modified_amr = amr_text.rstrip(')') + \"\\n    :time \\\"yesterday\\\")\"\n",
    "            return modified_amr, \"Added time circumstance 'yesterday'\"\n",
    "    \n",
    "    elif error_type == \"discourse\":\n",
    "        # Add a simple cause relation if not already present\n",
    "        if \":cause\" not in amr_text:\n",
    "            # Add before the last closing parenthesis\n",
    "            modified_amr = amr_text.rstrip(')') + \"\\n    :cause \\\"unknown\\\")\"\n",
    "            return modified_amr, \"Added cause relation 'unknown'\"\n",
    "    \n",
    "    elif error_type == \"out_of_article\":\n",
    "        # Add a simple comment that's clearly out of article\n",
    "        modified_amr = amr_text.rstrip(')') + \"\\n    :comment \\\"This is out of article information\\\")\"\n",
    "        return modified_amr, \"Added out of article comment\"\n",
    "    \n",
    "    # If all else fails, just return the original\n",
    "    return amr_text, \"No modification (fallback failed)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b881c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e9c38255e0478a8d07c652a00c4690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG3', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG3', 'z7')\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "Missing concept: ( z0 / mulai-01 :ARG0 ( z1 / orang :wiki \"Luis_Buñuel\" :name ( z2 / nama :op1 \"Bunuel\" ) ) :ARG1 ( z3 / sumbang-01 :ARG0 z1 :ARG1 ( z4 / artikel ) :ARG2 ( z5 / film :mod ( z6 / berbagai ) :mod ( z7 / periodik ) :mod ( z8 / terutama ) ) ) :mod ( z9 / juga ) :time ( z10 / asah-01 :ARG0 z1 :ARG1 ( z11 / keterampilan :poss z1 ) :ARG2 ( z12 / orang :ARG0-of ( z13 / punya-peran-org-91 :ARG2 ( z14 / ) ) ) ) ) \n",
      "ignoring secondary node contexts for 'z11'\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', '\"14:30\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', '\"14:30\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "cannot deinvert attribute: ('z5', ':part-of', 'z3')\n",
      "cannot deinvert attribute: ('z5', ':part-of', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z2')\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "Missing concept: ( z0 / perlu-01 :ARG0 ( z1 / kamu ) :ARG1 ( z2 / kembali-01 :ARG1 ( z3 / ambil-01 :ARG0 z1 :ARG1 ( z4 / debat-01 :mod ( z5 / ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z4', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':ARG1', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "Building failure\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 157, in decode_amr\n",
      "    graph = self._fix_and_make_graph(nodes)\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 442, in _fix_and_make_graph\n",
      "    graph = penman.decode(linearized + ' ')\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 191, in _decode\n",
      "    return codec.decode(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 56, in decode\n",
      "    tree = parse(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 34, in parse\n",
      "    return _parse(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 81, in _parse\n",
      "    node = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 163, in _parse_edge\n",
      "    raise tokens.error('Expected: SYMBOL, STRING, LPAREN', token=_next)\n",
      "penman.exceptions.DecodeError: \n",
      "  line 1\n",
      "    ( z0 / kontras-01 :ARG1 ( z1 / sebab-01 :ARG0 ( z2 / aku ) :ARG1 ( z3 / jadi-01 :ARG1 ( z4 / orang :mod ( z5 / homoseksual ) ) :ARG2 ( z6 / sehat-01 :polarity - :ARG1 z4 ) ) ) :ARG2 ( z7 / lihat-01 :polarity - :ARG0 z2 :ARG1 ( z8 / sesuatu :ARG1-of ( z9 / respon-01 ) ) :mod ( z10 / masih ) :topic ( z11 / sesuatu :ARG1-of ( z12 / kirim-01 :ARG0 ( z13 / orang :quant #267 ) ) ) ) ) \n",
      "                                                                                                                                                                                                                                                                                                                                                                                   ^\n",
      "DecodeError: Expected: SYMBOL, STRING, LPAREN\n",
      "\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z21', ':wiki', '\"Nineteen_Eighty-Four\"')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':wiki', '\"gerakan-politik\"')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / peristiwa :mod ( z2 / orang :wiki \"George_W._Bush\" :name ( z3 / nama :op1 \"Bush\" ) ) ) :op2 ( z4 / peristiwa :ARG1-of ( z5 / ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z1', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z32', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z32', ':ARG2', '-')\n",
      "ignoring secondary node contexts for 'z8'\n",
      "ignoring epigraph data for duplicate triple: ('z14', ':time', 'z1003')\n",
      "ignoring epigraph data for duplicate triple: ('z14', ':time', 'z1003')\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring secondary node contexts for 'z1003'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':wiki', '\"gerakan-politik\"')\n",
      "Missing concept: ( z0 / usia-01 :ARG1 ( z1 / kamu ) :ARG2 ( z2 / amr-tidak-diketahui ) :mod ( z3 / ) ) \n",
      "Missing concept: ( z0 / multikalimat :snt1 ( z1 / tetap-01 :ARG1 ( z2 / entitas-persentase :value 14 ) :mod ( z3 / masih ) ) :snt2 ( z4 / tambah-01 :ARG2 ( z5 / ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z2')\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z6')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z6')\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z7')\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring secondary node contexts for 'z7'\n",
      "ignoring epigraph data for duplicate triple: ('z20', ':ARG0', 'z12')\n",
      "ignoring epigraph data for duplicate triple: ('z20', ':ARG0', 'z12')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG0', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG0', 'z1')\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring secondary node contexts for 'z1'\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':day', '25')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':day', '25')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / tarik-01 :mode imperatif :ARG0 ( z2 / kamu ) :ARG2 ( z3 / atas :op1 ( z4 / kepala :part-of z2 ) ) ) :op2 ( z5 / segel-01 :mode imperatif :ARG0 z2 :ARG1-of ( z6 / erat-01 ) ) :op3 ( z7 / ambil-01 :mode imperatif :ARG0 z2 :ARG1 ( z8 / nap :quant ( z9 / banyak ) ) :mod ( z10 / dalam-dalam ) ) :op4 ( z11 / ikut-01 :mode imperatif :ARG0 ( z12 / kamu ) :ARG1 ( z13 / jalan :mod ( z14 / ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG1', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':ARG1', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z19', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG4', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG4', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG0', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z3')\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring secondary node contexts for 'z3'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':year', '11')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':year', '11')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':time', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z26', ':ARG0', 'z17')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':time', 'z5')\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring secondary node contexts for 'z5'\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':domain', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':domain', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':mod', 'z3')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z7', ':ARG1', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':ARG0', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z6', ':ARG0', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':ARG1', 'z1071')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z45', ':instance', None)\n",
      "Building failure\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 157, in decode_amr\n",
      "    graph = self._fix_and_make_graph(nodes)\n",
      "  File \"/home/jupyter-23522029/thesis/model_interface/tokenization_bart.py\", line 489, in _fix_and_make_graph\n",
      "    g = penman.decode(linearized)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 191, in _decode\n",
      "    return codec.decode(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/codec.py\", line 56, in decode\n",
      "    tree = parse(s)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 34, in parse\n",
      "    return _parse(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 81, in _parse\n",
      "    node = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 158, in _parse_edge\n",
      "    target = _parse_node(tokens)\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 130, in _parse_node\n",
      "    edges.append(_parse_edge(tokens))\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_parse.py\", line 145, in _parse_edge\n",
      "    role_token = tokens.expect('ROLE')\n",
      "  File \"/home/jupyter-23522029/.local/lib/python3.10/site-packages/penman/_lexer.py\", line 140, in expect\n",
      "    raise self.error(\n",
      "penman.exceptions.DecodeError: \n",
      "  line 151\n",
      "                                                                                                                     / thing\n",
      "                                                                                                                     ^\n",
      "DecodeError: Expected: ROLE\n",
      "\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':domain', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':domain', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z16', ':ARG0', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z16', ':ARG0', 'z7')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':wiki', '\"gerakan-politik\"')\n",
      "ignoring secondary node contexts for 'z11'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG1', 'z4')\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring secondary node contexts for 'z4'\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG2', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':ARG2', 'z2')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z9', ':ARG1', 'z5')\n",
      "ignoring epigraph data for duplicate triple: ('z72', ':mode', 'ekspresif-01')\n",
      "ignoring epigraph data for duplicate triple: ('z72', ':mode', 'ekspresif-01')\n",
      "Missing concept: ( z0 / Santai-01 :mode imperatif :ARG0 ( z1 / kamu ) :mod ( z2 / saja ) :ARG1-of ( z3 / sebab-01 :ARG0 ( z4 / punya-derajat-91 :ARG1 z1 :ARG2 ( z5 / kuat-01 :ARG1 z1 ) :ARG3 ( z6 / terlalu ) ) :condition ( z7 / laku-01 :ARG0 z1 :ARG1 ( z8 / sesuatu :ARG1-of ( z9 / serius-01 ) ) :time ( z10 / entitas-tanggal :weekday ( z11 / ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z13', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z13', ':ARG2', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':ARG1', 'z1')\n"
     ]
    }
   ],
   "source": [
    "# Batch processing approach\n",
    "from AMRFactGenerator import AMRFactDynamicGenerator\n",
    "from penman.models.amr import model as amr_model\n",
    "import penman\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize error generator\n",
    "error_generator = AMRFactDynamicGenerator(model=amr_model)\n",
    "\n",
    "# Clone the dataframe\n",
    "df_augmented = df_train_concat_amr3.copy()\n",
    "\n",
    "# Add new columns\n",
    "df_augmented['generated'] = None\n",
    "df_augmented['augmented'] = None\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Process in batches\n",
    "total_batches = (len(df_augmented) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "results = []\n",
    "\n",
    "for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "    # Get batch slice\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df_augmented))\n",
    "    batch_indices = list(range(start_idx, end_idx))\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        send_telegram_message(f\"{batch_idx} processed\")\n",
    "    # Process each row in the batch\n",
    "    batch_results = []\n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            # Generate AMR from text\n",
    "            text = df_augmented.iloc[idx]['id']  # Use Indonesian text\n",
    "            amr_graph = text_to_amr(text)\n",
    "            amr_graph.metadata = {}  # Reset metadata\n",
    "            \n",
    "            # Store the generated AMR\n",
    "            try:\n",
    "                amr_text = penman.encode(amr_graph)\n",
    "                df_augmented.at[idx, 'generated'] = amr_text\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding generated AMR for row {idx}: {e}\")\n",
    "                batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "                continue\n",
    "            \n",
    "            # Generate all error types with fallbacks\n",
    "            augmentations = {}\n",
    "            for error_type in [\"predicate\", \"entity\", \"circumstance\", \"discourse\", \"out_of_article\"]:\n",
    "                try:\n",
    "                    # Generate error\n",
    "                    modified_graph, description = error_generator.introduce_error(copy.deepcopy(amr_graph), error_type)\n",
    "                    modified_amr = penman.encode(modified_graph)\n",
    "                    \n",
    "                    augmentations[error_type] = {\n",
    "                        'amr': modified_amr,\n",
    "                        'description': description\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    # Use fallback if needed\n",
    "                    try:\n",
    "                        fallback_amr, fallback_desc = create_fallback_error(amr_text, error_type)\n",
    "                        augmentations[error_type] = {\n",
    "                            'amr': fallback_amr,\n",
    "                            'description': f\"Fallback {error_type} error: {fallback_desc}\"\n",
    "                        }\n",
    "                    except:\n",
    "                        augmentations[error_type] = {\n",
    "                            'amr': amr_text,\n",
    "                            'description': f\"Error generating {error_type} error: {str(e)}\"\n",
    "                        }\n",
    "            \n",
    "            # Update the dataframe\n",
    "            df_augmented.at[idx, 'augmented'] = augmentations\n",
    "            batch_results.append({'idx': idx, 'status': 'success'})\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    # Add batch results to overall results\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Optional: Save checkpoint after each batch\n",
    "    if batch_idx % 5 == 0:\n",
    "        df_augmented.to_json(f'augmented_amr_checkpoint_{batch_idx}.json', orient='records')\n",
    "\n",
    "# Final save\n",
    "df_augmented.to_json('augmented_amr_dataset_3.0.json', orient='records')\n",
    "\n",
    "# Show stats\n",
    "print(f\"Successfully processed {sum(1 for r in results if r['status'] == 'success')} rows\")\n",
    "print(f\"Errors in {sum(1 for r in results if r['status'] == 'error')} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa026880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Predicate Error: Changed sense from 'utama-01' to 'utama-04'\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df_augmented.iloc[0]['augmented']\n",
    "a['predicate']['amr']\n",
    "a['predicate']['description']\n",
    "# a['entity']['amr']\n",
    "# a['entity']['description']\n",
    "# a['circumstance']['amr']\n",
    "# a['circumstance']['description']\n",
    "# a['discourse']['amr']\n",
    "# a['discourse']['description']\n",
    "# a['out_of_article']['amr']\n",
    "# a['out_of_article']['description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f1d5-bdcf-407b-a1b5-2bc722b4341d",
   "metadata": {},
   "source": [
    "### data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb002ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07c3e61c0db431ca4bb859a8f041430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing concept: ( z0 / proses-01 :ARG1 ( z1 / mengembangkan-01 :ARG1 ( z2 / vaksin :wiki \"COVID-19\" :name ( z3 / nama :op1 \"COVID-19\" ) ) ) :ARG1-of ( z4 / melibatkan-01 :ARG2 ( z5 / vaksin :quant ( z6 / berbagai ) :ARG2-of ( z7 / golong-91 :ARG1 ( z8 / dan :op1 ( z9 / vaksin :mod ( z10 / virus :ARG1-of ( z11 / aktiv-01 ) ) ) :op2 ( z12 / vaksin :ARG1-of ( z13 / berbasis-01 :ARG2 ( z14 / protein ) ) ) :op3 ( z15 / vaksin :mod ( z16 / vektor ) ) :op4 ( z17 / vaksin :mod ( z18 / ) ) ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z8', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z8', ':ARG1', 'z4')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '\"telepon\"')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':wiki', '\"telepon\"')\n",
      "Missing concept: ( z0 / dan :op1 ( z1 / tutup-01 :ARG1 ( z2 / foto ) :time ( z3 / telah ) ) :op2 ( z4 / tunjuk-01 :ARG0 ( z5 / gambar :mod ( z6 / asli ) ) :ARG1 ( z7 / laku-01 :ARG0 ( z8 / mereka ) :ARG1 ( z9 / telepon-01 :ARG0 z8 :ARG1 ( z10 / dan :op1 ( z11 / orang :wiki \"Tiongkok\" :name ( z12 / nama :op1 \"Joko\" :op2 \"Widodo\" ) :ARG0-of ( z13 / punya-peran-org-91 :ARG2 ( z14 / presiden ) ) ) :op2 ( z15 / orang :wiki \"Iriana\" :name ( z16 / nama :op1 \"Iriana\" ) :ARG0-of ( z17 / punya-peran-rel-91 :ARG1 z10 :ARG2 ( z18 / istri ) ) ) ) :manner ( z19 / video ) ) :time ( z20 / meraya-01 :ARG0 z8 :ARG1 ( z1000 / entitas-tanggal :month 1 :day 14 ) ) ) ) :ARG1-of ( z1001 / temu-01 :ARG0 ( z1002 / tim :mod ( z1003 / fakta ) :mod ( z1004 / ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z12', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z0', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z5', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z5', ':wiki', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z4', ':wiki', '\"perusahaan\"')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z12', ':year', '20')\n",
      "Missing concept: ( z0 / tunjuk-01 :polarity - :ARG0 ( z1 / video :mod ( z2 / ini ) ) :ARG1 ( z3 / mata :part-of ( z4 / orang ) :ARG1-of ( z5 / benar-01 ) ) :ARG1-of ( z6 / sebab-01 :ARG0 ( z7 / punya-mod-91 :ARG1 z1 :ARG2 ( z8 / karya :mod ( z9 / seni :mod ( z10 / ) ) ) ) ) ) \n",
      "ignoring epigraph data for duplicate triple: ('z11', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z11', ':year', '20')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z3', ':polarity', '-')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '26')\n",
      "ignoring epigraph data for duplicate triple: ('z10', ':year', '26')\n"
     ]
    }
   ],
   "source": [
    "import penman\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Clone the dataframe\n",
    "df_amred = df_train_set_hki.copy()\n",
    "\n",
    "# Add new columns\n",
    "df_amred['amr_claim'] = None\n",
    "df_amred['amr_llm_key_points'] = None\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Process in batches\n",
    "total_batches = (len(df_amred) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "results = []\n",
    "\n",
    "for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "    # Get batch slice\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df_amred))\n",
    "    batch_indices = list(range(start_idx, end_idx))\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        send_telegram_message(f\"{batch_idx} processed\")\n",
    "    # Process each row in the batch\n",
    "    batch_results = []\n",
    "    for idx in batch_indices:\n",
    "        try:\n",
    "            # Generate AMR from text\n",
    "            text = df_amred.iloc[idx]['Claim']  \n",
    "            amr_graph = text_to_amr(text)\n",
    "            amr_graph.metadata = {} \n",
    "\n",
    "            text = df_amred.iloc[idx]['LLM_key_points']\n",
    "            text_list = text.split(\"\\n- \")\n",
    "            text_list[0] = text_list[0].lstrip(\"- \")\n",
    "\n",
    "            amr_list = []\n",
    "            for a_text in text_list:\n",
    "                try:\n",
    "                    amr_graph = text_to_amr(a_text)\n",
    "                    amr_graph.metadata = {}\n",
    "                    amr_text = penman.encode(amr_graph)\n",
    "                    amr_list.append(amr_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"error encoding {e} for {a_text}\")\n",
    "            \n",
    "                \n",
    "            \n",
    "            # Store the generated AMR\n",
    "            try:\n",
    "                amr_text = penman.encode(amr_graph)\n",
    "                df_amred.at[idx, 'amr_claim'] = amr_text\n",
    "                df_amred.at[idx, 'amr_llm_key_points'] = amr_list\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding generated AMR for row {idx}: {e}\")\n",
    "                batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "                continue\n",
    "            \n",
    "            # Update the dataframe\n",
    "            batch_results.append({'idx': idx, 'status': 'success'})\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_results.append({'idx': idx, 'status': 'error', 'error': str(e)})\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    # Add batch results to overall results\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Optional: Save checkpoint after each batch\n",
    "    if batch_idx % 5 == 0:\n",
    "        df_amred.to_json(f'IndoHoax_amr_checkpoint_{batch_idx}.json', orient='records')\n",
    "\n",
    "# Final save\n",
    "df_amred.to_json('indo_dax_amr.json', orient='records')\n",
    "send_telegram_message(f\"finish all process\")\n",
    "\n",
    "# Show stats\n",
    "print(f\"Successfully processed {sum(1 for r in results if r['status'] == 'success')} rows\")\n",
    "print(f\"Errors in {sum(1 for r in results if r['status'] == 'error')} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c332f8d-5ad7-4b11-b04a-f20cd7d697c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
